---
name: Agent Workflow Overhaul
overview: Overhaul the agent loop to follow modern production agent patterns (Claude Code, Anthropic guide, OpenAI guide). Parallel tool execution, microcompaction context management, prompt caching, streaming tool status events, and 5 new lightweight tools.
todos:
  - id: parallel-tools
    content: "Implement parallel tool execution with asyncio.gather in agent.py (both run() and run_stream())"
    status: pending
  - id: microcompaction
    content: "Implement microcompaction: save large tool results to disk, keep truncated reference inline. Keep 'hot tail' of recent results full-size, compress older ones."
    status: pending
  - id: prompt-caching
    content: "Cache system prompt string and tool schemas once at module level (not per-request). Ensure byte-stable prefix for llama.cpp KV cache reuse."
    status: pending
  - id: tool-status-events
    content: "Add ToolStatusEvent stream type and yield it during tool execution in run_stream(). Wire through chat.py SSE with distinct event type."
    status: pending
  - id: reduce-max-iter
    content: "Reduce AGENT_MAX_ITERATIONS from 15 to 8 in config.py"
    status: pending
  - id: tool-file-reader
    content: "Create tools/file_ops/reader.py — read files from uploads/scratch with offset+limit support"
    status: pending
  - id: tool-file-writer
    content: "Create tools/file_ops/writer.py — write/append to files in scratch workspace"
    status: pending
  - id: tool-file-navigator
    content: "Create tools/file_ops/navigator.py — list_directory + find_files (glob) in uploads/scratch"
    status: pending
  - id: tool-shell-exec
    content: "Create tools/shell/tool.py — sandboxed subprocess execution in scratch workspace"
    status: pending
  - id: tool-memory
    content: "Create tools/memory/tool.py — per-user persistent JSON key-value store with set/get/delete/list"
    status: pending
  - id: tool-schemas-dispatch
    content: "Add all 5 new tool schemas to tools_config.py, dispatch cases to agent.py, entries in config.py AVAILABLE_TOOLS"
    status: pending
  - id: system-prompt
    content: "Rewrite prompts/system.txt as structured ACI prompt following Anthropic's tool documentation guidance for all 8 tools"
    status: pending
isProject: false
---

# Agent Workflow Overhaul

This plan is grounded in three authoritative sources:
- **Anthropic "Building Effective Agents" (Dec 2024)**: Single agent loop, simple composable patterns, invest in ACI (Agent-Computer Interface) tool documentation
- **Claude Code architecture (2025)**: `while(tool_call)` loop, three-tier microcompaction, structured system prompt, file/shell/search/memory tools
- **OpenAI "Practical Guide to Building Agents" (2025)**: Single agent with tools, guardrails, streaming observability, 3-5 tools per agent optimal

## Current State

[backend/agent.py](backend/agent.py) is a single `while(tool_call)` loop -- this core pattern is correct and matches all three guides. The problems are operational:

- Tools execute **serially** (Anthropic recommends parallelization as a core pattern)
- Context grows **unbounded** (Claude Code uses 3-tier microcompaction to prevent this)
- System prompt rebuilt **from disk every call** (kills llama.cpp KV cache prefix reuse)
- No **streaming visibility** during tool execution (OpenAI/Anthropic SDKs both emit tool status events)
- Only 3 tools, all heavyweight (Claude Code has lightweight file/shell/memory tools)

## Proposed Architecture

```mermaid
flowchart TD
    UserMsg[User Message] --> AgentLoop

    subgraph AgentLoop [Agent Loop]
        direction TB
        LLMCall["LLM Call\n(cached system prefix + tool schemas)"] --> HasTools{tool_calls?}
        HasTools -->|No| ReturnText[Return text response]
        HasTools -->|Yes| Parallel["Execute tools in PARALLEL\n(asyncio.gather)"]
        Parallel --> Microcompact["Microcompact:\n- Truncate current results if large\n- Compress older tool results to references"]
        Microcompact --> StreamStatus["Yield ToolStatusEvent\n(tool name, status, duration)"]
        StreamStatus --> LLMCall
    end

    AgentLoop --> Response[SSE Stream to Client]
```

---

## 1. Parallel Tool Execution

**Source**: Anthropic "Building Effective Agents" lists Parallelization as one of 5 core workflow patterns. LLMCompiler research: 3.7x latency reduction.

**File**: [backend/agent.py](backend/agent.py) -- `run()` lines 246-248, `run_stream()` lines 294-296

Replace serial `for` loop with `asyncio.gather`:

```python
results = await asyncio.gather(
    *(self.execute_tool(tc.function.name, tc.function.arguments)
      for tc in response.tool_calls)
)
for tc, result in zip(response.tool_calls, results):
    msgs.append(self._build_tool_result_msg(tc, result))
```

Apply same change in both `run()` and `run_stream()`.

---

## 2. Microcompaction (Context Management)

**Source**: Claude Code uses a three-tier system (microcompaction / auto-compaction / manual compaction). Research paper "The Complexity Trap" (2025) shows simple observation masking matches LLM summarization at half the cost.

We implement the **microcompaction tier** -- the most impactful tier that requires zero extra LLM calls:

**File**: [backend/agent.py](backend/agent.py)

**How it works:**
- Each tool result has a **size budget** (configurable per tool in config.py)
- When a tool result exceeds its budget, the full result is saved to disk at `data/tool_results/{session_id}/{call_id}.json` and the inline content is replaced with a truncated version + disk reference
- After each iteration, all tool results from **previous iterations** (not the current one) are compressed to a one-line summary: `"[tool_name completed: {brief_summary}]"`. Only the current iteration's results stay full-size ("hot tail" pattern from Claude Code)
- This prevents the quadratic context blowup that is the primary performance killer

**New config values** in [config.py](config.py):
```python
TOOL_RESULT_BUDGET = {
    "websearch": 2000,
    "python_coder": 5000,
    "rag": 3000,
    "file_reader": 4000,
    "file_writer": 500,
    "file_navigator": 2000,
    "shell_exec": 3000,
    "memory": 500,
}
TOOL_RESULTS_DIR = Path("data/tool_results")
```

**New methods in AgentLoop:**
- `_save_tool_result_to_disk(session_id, call_id, result)` -- persist full result
- `_truncate_tool_result(tool_name, content)` -- apply per-tool budget
- `_compress_old_iterations(msgs, current_iteration)` -- replace old tool results with summaries

---

## 3. Prompt Caching for llama.cpp KV Reuse

**Source**: Claude Code achieves 92% prefix reuse rate. llama.cpp automatically caches KV states for byte-identical prefixes.

**Files**: [backend/agent.py](backend/agent.py), [config.py](config.py)

The key insight: llama.cpp reuses cached computation when the **beginning** of the prompt is byte-identical. Currently, `_build_system_prompt()` re-reads from disk and `_get_tool_schemas()` re-builds from dict every call, producing potentially different byte orderings.

**Changes:**
- Read system prompt at **module load time** and store in a module-level constant
- Build tool schemas at **module load time** and freeze as a tuple (immutable, stable order)
- In `AgentLoop.__init__`, reference these cached values instead of rebuilding
- The message list structure `[system_prompt, ...history]` ensures the system prompt is always the prefix, maximizing KV cache hits across iterations and across requests

---

## 4. Streaming Tool Status Events

**Source**: OpenAI Agents SDK emits `tool.call` / `tool.result` events. Claude Agent SDK emits `StreamEvent` objects for tool execution. Research: streaming improves perceived performance by 40%.

**Files**: [backend/core/llm_backend.py](backend/core/llm_backend.py), [backend/agent.py](backend/agent.py), [backend/api/routes/chat.py](backend/api/routes/chat.py), [backend/models/schemas.py](backend/models/schemas.py)

Add `ToolStatusEvent` dataclass in [backend/core/llm_backend.py](backend/core/llm_backend.py):
```python
@dataclass
class ToolStatusEvent(StreamEvent):
    tool_name: str = ""
    tool_call_id: str = ""
    status: str = ""        # "started" | "completed" | "failed"
    duration: float = 0.0   # seconds, only set on completed/failed
```

In `run_stream()`, yield `ToolStatusEvent` before and after each tool execution.

In [backend/api/routes/chat.py](backend/api/routes/chat.py), serialize to SSE as:
```json
{"object": "tool.status", "tool_name": "websearch", "status": "started"}
```

In [backend/models/schemas.py](backend/models/schemas.py), add `ToolStatusChunk` Pydantic model for serialization.

---

## 5. Reduce Default Max Iterations

**Source**: Budget-aware agent research (BATS, 2025) shows diminishing returns beyond 5-8 iterations.

**File**: [config.py](config.py)

Change `AGENT_MAX_ITERATIONS` from 15 to 8. Most real tasks complete in 1-3 iterations. The old value of 15 allows runaway token costs with no proportional benefit.

---

## 6. New Tools

**Source**: Claude Code's tool categories are: file operations (read, write, edit), search (glob, grep), execution (bash/shell), and memory. OpenAI's guide recommends 3-5 tools per agent; we'll have 8 total but with distinct, non-overlapping responsibilities and clear descriptions (per Anthropic's ACI guidance: "spend as much effort on agent-computer interfaces as on human-computer interfaces").

All tools follow existing pattern: class returning `{"success": bool, ...}`, schema in [tools_config.py](tools_config.py), dispatch in [backend/agent.py](backend/agent.py).

### 6a. `file_reader` -- Read file contents

**New file**: `tools/file_ops/reader.py`

Parameters: `path` (relative to uploads or scratch), `offset` (line number, optional), `limit` (line count, optional)

Returns: `{"success": true, "content": "...", "lines": N, "size": N, "truncated": bool}`

- Resolves path within `data/uploads/{username}/` or `data/scratch/{session_id}/`
- Size cap: 50KB (return truncated=true if exceeded)
- Supports text formats: txt, md, json, csv, py, js, html, xml, yaml, log, etc.
- Use absolute paths for unambiguous resolution (per Anthropic SWE-bench lesson: "we changed the tool to always require absolute filepaths -- the model used this flawlessly")

### 6b. `file_writer` -- Write/append file contents

**New file**: `tools/file_ops/writer.py`

Parameters: `path` (relative to scratch), `content` (string), `mode` ("write" | "append")

Returns: `{"success": true, "path": "...", "bytes_written": N}`

- Only writes to `data/scratch/{session_id}/` (not uploads -- those are read-only)
- Creates parent directories automatically

### 6c. `file_navigator` -- List and search files

**New file**: `tools/file_ops/navigator.py`

Parameters: `operation` ("list" | "find"), `path` (directory for list, optional), `pattern` (glob for find, optional)

Returns: `{"success": true, "files": [{"name": "...", "size": N, "modified": "...", "is_dir": bool}]}`

- Operates on both `data/uploads/{username}/` and `data/scratch/{session_id}/`
- `list`: returns directory entries with metadata
- `find`: glob search (e.g., `"*.csv"`, `"**/*.py"`)

### 6d. `shell_exec` -- Execute shell commands

**New file**: `tools/shell/tool.py`

Parameters: `command` (string), `timeout` (int, default 30), `working_directory` (relative to scratch, optional)

Returns: `{"success": true, "stdout": "...", "stderr": "...", "exit_code": N, "duration": N}`

- Working directory restricted to `data/scratch/{session_id}/`
- Timeout enforced via subprocess
- stdout/stderr capped at configurable size

### 6e. `memory` -- Persistent cross-session key-value store

**New file**: `tools/memory/tool.py`
**Storage**: `data/memory/{username}.json`

Parameters: `operation` ("set" | "get" | "delete" | "list"), `key` (string, optional), `value` (string, optional)

Returns: `{"success": true, "value": "..."}` or `{"success": true, "keys": [...]}`

- Scoped per **user** (not per session) -- persists across sessions
- Uses JSON file with `FileLock` (same concurrency pattern as `ConversationStore`)
- Enables the agent to remember user preferences, project context, frequently used settings

---

## 7. System Prompt (Agent-Computer Interface)

**Source**: Anthropic's guide says "we actually spent more time optimizing our tools than the overall prompt" and recommends treating tool design as ACI (Agent-Computer Interface) engineering. Claude Code uses structured sections with explicit tool guidance.

**File**: [prompts/system.txt](prompts/system.txt)

Rewrite as a structured prompt with these sections:

1. **Identity and behavior** -- who you are, how you respond
2. **Tool usage policy** -- when to use tools vs. answer directly; prefer lightweight tools for simple ops
3. **Tool reference** -- one paragraph per tool with: purpose, when to use, when NOT to use, example usage
4. **Tool selection hierarchy**:
   - Reading files? Use `file_reader`, not `python_coder`
   - Listing/finding files? Use `file_navigator`, not `shell_exec` with `ls`
   - Running commands? Use `shell_exec`, not `python_coder` with `subprocess`
   - Simple file creation? Use `file_writer`, not `python_coder`
   - Remembering preferences? Use `memory`
   - Complex computation/data analysis? Use `python_coder`
   - Current information? Use `websearch`
   - Document retrieval? Use `rag`
5. **Response guidelines** -- synthesize tool results, don't dump raw output

---

## Files Changed Summary

**Modified** (7 files):
- [backend/agent.py](backend/agent.py) -- parallel execution, microcompaction, tool status events, new tool dispatch, cached prompt/schemas
- [backend/core/llm_backend.py](backend/core/llm_backend.py) -- add `ToolStatusEvent` dataclass
- [backend/api/routes/chat.py](backend/api/routes/chat.py) -- serialize `ToolStatusEvent` in SSE stream
- [backend/models/schemas.py](backend/models/schemas.py) -- add `ToolStatusChunk` pydantic model
- [tools_config.py](tools_config.py) -- 5 new tool schemas
- [config.py](config.py) -- new tools in AVAILABLE_TOOLS, tool result budgets, tool results dir, max iterations
- [prompts/system.txt](prompts/system.txt) -- complete rewrite as structured ACI prompt

**New** (8 files):
- `tools/file_ops/__init__.py`
- `tools/file_ops/reader.py`
- `tools/file_ops/writer.py`
- `tools/file_ops/navigator.py`
- `tools/shell/__init__.py`
- `tools/shell/tool.py`
- `tools/memory/__init__.py`
- `tools/memory/tool.py`

