{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Orchestrated Report Generator (using Agentic LLM API developed by SiHun Lee, CAE G., MX div., SEC.)\n",
    "\n",
    "This notebook uses **LLM API's agentic capabilities** to automatically generate comprehensive PDF and PowerPoint reports from warpage data.\n",
    "\n",
    "## How It Works\n",
    "\n",
    "1. **Phase 1:** Analyze data → finds outliers, calculates statistics\n",
    "2. **Phase 2:** Generate charts → uses Phase 1 findings (not raw files)\n",
    "3. **Phase 3:** Build PDF Report → comprehensive, beautiful PDF document\n",
    "4. **Phase 4:** Build PowerPoint → comprehensive presentation with same content\n",
    "\n",
    "**Key Advantage:** Each phase reuses conversation memory, avoiding redundant file processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import time\n",
    "from IPython.display import display, Latex\n",
    "\n",
    "# Build Universal LLM API Client\n",
    "class LLMApiClient:\n",
    "    def __init__(self, base_url: str, timeout: float = 360000.0):\n",
    "        self.base_url = base_url.rstrip(\"/\")\n",
    "        self.token = None\n",
    "        self.timeout = httpx.Timeout(50.0, read=timeout, write=timeout, pool=timeout)\n",
    "\n",
    "    def _headers(self):\n",
    "        return {\"Authorization\": f\"Bearer {self.token}\"} if self.token else {}\n",
    "\n",
    "    def login(self, username: str, password: str):\n",
    "        r = httpx.post(f\"{self.base_url}/api/auth/login\", \n",
    "                      json={\"username\": username, \"password\": password}, timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        self.token = r.json()[\"access_token\"]\n",
    "        return r.json()\n",
    "\n",
    "    def list_models(self):\n",
    "        r = httpx.get(f\"{self.base_url}/v1/models\", headers=self._headers(), timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "\n",
    "    def chat_new(self, model: str, user_message: str, agent_type: str = \"auto\", files: list = None):\n",
    "        messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "        data = {\"model\": model, \"messages\": json.dumps(messages), \"agent_type\": agent_type}\n",
    "        \n",
    "        files_to_upload = []\n",
    "        if files:\n",
    "            for file_path in files:\n",
    "                f = open(file_path, \"rb\")\n",
    "                files_to_upload.append((\"files\", (Path(file_path).name, f)))\n",
    "        \n",
    "        try:\n",
    "            r = httpx.post(f\"{self.base_url}/v1/chat/completions\", data=data,\n",
    "                          files=files_to_upload if files_to_upload else None,\n",
    "                          headers=self._headers(), timeout=self.timeout)\n",
    "            r.raise_for_status()\n",
    "            result = r.json()\n",
    "            return result[\"choices\"][0][\"message\"][\"content\"], result[\"x_session_id\"]\n",
    "        finally:\n",
    "            for _, (_, f) in files_to_upload:\n",
    "                f.close()\n",
    "\n",
    "    def chat_continue(self, model: str, session_id: str, user_message: str, \n",
    "                     agent_type: str = \"auto\", files: list = None):\n",
    "        messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "        data = {\"model\": model, \"messages\": json.dumps(messages), \n",
    "                \"session_id\": session_id, \"agent_type\": agent_type}\n",
    "        \n",
    "        files_to_upload = []\n",
    "        if files:\n",
    "            for file_path in files:\n",
    "                f = open(file_path, \"rb\")\n",
    "                files_to_upload.append((\"files\", (Path(file_path).name, f)))\n",
    "        \n",
    "        try:\n",
    "            r = httpx.post(f\"{self.base_url}/v1/chat/completions\", data=data,\n",
    "                          files=files_to_upload if files_to_upload else None,\n",
    "                          headers=self._headers(), timeout=self.timeout)\n",
    "            r.raise_for_status()\n",
    "            result = r.json()\n",
    "            return result[\"choices\"][0][\"message\"][\"content\"], result[\"x_session_id\"]\n",
    "        finally:\n",
    "            for _, (_, f) in files_to_upload:\n",
    "                f.close()\n",
    "\n",
    "    def get_session_artifacts(self, session_id: str):\n",
    "        \"\"\"Get list of files generated during the session\"\"\"\n",
    "        r = httpx.get(f\"{self.base_url}/api/chat/sessions/{session_id}/artifacts\",\n",
    "                     headers=self._headers(), timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "\n",
    "    def download_artifact(self, session_id: str, filename: str, save_to: str = None):\n",
    "        \"\"\"\n",
    "        Download a generated artifact file to local disk.\n",
    "        \n",
    "        Args:\n",
    "            session_id: The session ID that generated the file\n",
    "            filename: Name of the file to download (can include subdirectory, e.g., 'temp_charts/chart.png')\n",
    "            save_to: Local path to save the file (default: current directory with original filename)\n",
    "        \n",
    "        Returns:\n",
    "            str: Path to the downloaded file\n",
    "        \n",
    "        Example:\n",
    "            client.download_artifact(session_id, \"Warpage_Report_20250126.pptx\", \"./downloads/report.pptx\")\n",
    "        \"\"\"\n",
    "        r = httpx.get(\n",
    "            f\"{self.base_url}/api/chat/sessions/{session_id}/artifacts/{filename}\",\n",
    "            headers=self._headers(),\n",
    "            timeout=60.0\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "        \n",
    "        # Determine local save path\n",
    "        if save_to is None:\n",
    "            save_to = Path(filename).name  # Use just the filename, not subdirectory\n",
    "        \n",
    "        # Create parent directories if needed\n",
    "        save_path = Path(save_to)\n",
    "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Write file content\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "        \n",
    "        return str(save_path)\n",
    "\n",
    "# Configuration\n",
    "API_BASE_URL = 'http://localhost:10007'\n",
    "USERNAME = \"admin\"\n",
    "PASSWORD = \"administrator\"\n",
    "\n",
    "# Initialize and login\n",
    "client = LLMApiClient(API_BASE_URL, timeout=36000.0)# 10 hours\n",
    "client.login(USERNAME, PASSWORD)\n",
    "models = client.list_models()\n",
    "MODEL = models[\"data\"][0][\"id\"]\n",
    "\n",
    "print(f\"✓ Logged in as: {USERNAME}\")\n",
    "print(f\"✓ Using model: {MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your data files\n",
    "stats_paths = [\n",
    "    Path(\"B8_1021_stats.json\"),\n",
    "    Path(\"B8_1027_stats.json\"),\n",
    "]\n",
    "\n",
    "# Verify files exist\n",
    "print(f\"Configured {len(stats_paths)} data file(s):\\n\")\n",
    "for i, path in enumerate(stats_paths, 1):\n",
    "    if path.exists():\n",
    "        size_kb = path.stat().st_size / 1024\n",
    "        print(f\"  [{i}] {path.name} ({size_kb:.1f} KB) - ✓\")\n",
    "    else:\n",
    "        print(f\"  [{i}] {path.name} - ✗ NOT FOUND\")\n",
    "\n",
    "file_paths_str = [str(p) for p in stats_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Phase 1: Data Analysis\n",
    "\n",
    "The AI will analyze your data and identify key patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_prompt = f\"\"\"\n",
    "Analyze {len(stats_paths)} warpage measurement JSON files attached.\n",
    "\n",
    "Input Data Structure:\n",
    "- Each file contain warpage statistics per PCB board\n",
    "- Statistics: min, max, range (warpage value), mean, median, std, skewness, kurtosis\n",
    "- PCA values (pc1, pc2) calculated within each source_pdf\n",
    "- Filenames contain acquisition date/time (e.g., 1021 = October 21th)\n",
    "- Note that usually, mean, median is not important. To assess warpage, range is the single most important feature.\n",
    "\n",
    "Tasks:\n",
    "1. Calculate overall statistics (mean, std, min, max of range across all files)\n",
    "2. Identify PCA-based outliers using pc1, pc2 values. Look for PCA values that are quite a far from others\n",
    "3. Compare production dates - which is better quality and why?\n",
    "4. List specific outlier filenames with reasons\n",
    "5. Save your results to a numpy array locally\n",
    "\n",
    "**Required Output:**\n",
    "- Total measurements count\n",
    "- Outlier list with full filenames\n",
    "- Production date comparison (winner + reason)\n",
    "- Key concerns or patterns\n",
    "\n",
    "Think HARD!\n",
    "\"\"\"\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 1: DATA ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "start = time.time()\n",
    "analysis_result, session_id = client.chat_new(\n",
    "    MODEL, analysis_prompt, agent_type=\"auto\", files=file_paths_str\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Analysis completed in {time.time() - start:.1f}s\\n\")\n",
    "print(\"=\" * 80)\n",
    "display(Latex(analysis_result))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Phase 2: Generate Visualizations\n",
    "\n",
    "**Key:** AI reuses Phase 1 findings from conversation memory (not raw files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_prompt = f\"\"\"\n",
    "**PRIORITY: Use your Phase 1 analysis from conversation memory and saved numpy array.**\n",
    "\n",
    "In Phase 1, you already:\n",
    "- Analyzed {len(stats_paths)} datasets and loaded all data\n",
    "- Identified PCA outliers with pc1, pc2 values\n",
    "- Compared production dates\n",
    "- Listed specific outlier filenames\n",
    "\n",
    "**Avoid re-analyze raw files if possible. Use your Phase 1 findings and file.**\n",
    "Files attached are ONLY for verification if needed.\n",
    "\n",
    "**Task:** Create visualizations and classify outliers\n",
    "\n",
    "**Outlier Classification:**\n",
    "- **BAD outliers:** High mean/std/range (critical quality issues)\n",
    "- **GOOD outliers:** Unusual PCA position but acceptable metrics\n",
    "- **Normal:** Within PCA cluster, standard metrics\n",
    "\n",
    "**Required Charts** (save to temp_charts/):\n",
    "1. `pca_outliers_classified.png` - PC1 vs PC2 scatter (Blue=normal, Orange=good outlier, RED=bad outlier)\n",
    "2. `bad_outliers_detail.png` - Bar chart comparing bad outliers vs average\n",
    "3. `production_comparison.png` - Production date quality comparison\n",
    "4. Additional charts as appropriate (distributions, trends, control charts, etc.)\n",
    "\n",
    "**Style:** 300 DPI, seaborn whitegrid, professional colors\n",
    "\n",
    "**Required Output:**\n",
    "- List of generated chart files\n",
    "- Bad outlier summary (file IDs + reasons)\n",
    "- Production date insights\n",
    "\n",
    "THINK HARD!\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 2: VISUALIZATION GENERATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "start = time.time()\n",
    "viz_result, _ = client.chat_continue(\n",
    "    MODEL, session_id, viz_prompt, agent_type=\"auto\", files=file_paths_str\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Visualizations completed in {time.time() - start:.1f}s\\n\")\n",
    "print(\"=\" * 80)\n",
    "display(Latex(viz_result))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Phase 3: PDF Report Assembly\n",
    "\n",
    "**Key:** AI uses Phase 1 & 2 findings from conversation memory to create a beautiful, comprehensive PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total file count\n",
    "total_files = 0\n",
    "for path in stats_paths:\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        total_files += len(data.get('files', []))\n",
    "\n",
    "# Phase 3: PDF Report Generation\n",
    "pdf_prompt = f\"\"\"\n",
    "**PRIORITY: Use Phase 1 & 2 findings from conversation memory and files.**\n",
    "\n",
    "You have:\n",
    "- Phase 1: Statistics, outlier IDs, production date comparison\n",
    "- Phase 2: Images useful for report generation, bad outlier classifications\n",
    "\n",
    "**Avoid re-analyze raw files if possible. Use conversation context.**\n",
    "Files attached are ONLY for verification if needed.\n",
    "\n",
    "**Task:** Create comprehensive, beautiful PDF report using ReportLab library.\n",
    "\n",
    "**CRITICAL: Use the following structured template approach to ensure consistent formatting**\n",
    "\n",
    "## Installation\n",
    "```python\n",
    "# Ensure ReportLab is installed\n",
    "import subprocess\n",
    "subprocess.run(['pip', 'install', 'reportlab'], check=False)\n",
    "```\n",
    "\n",
    "## PDF Configuration (MUST USE THESE EXACT SETTINGS)\n",
    "\n",
    "```python\n",
    "from reportlab.lib.pagesizes import A4\n",
    "from reportlab.lib.units import cm\n",
    "from reportlab.lib import colors\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, PageBreak, Image, Table, TableStyle\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.lib.enums import TA_CENTER, TA_LEFT, TA_JUSTIFY\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Page setup - CONSISTENT A4 PORTRAIT ONLY\n",
    "PAGE_WIDTH, PAGE_HEIGHT = A4  # 595.27 x 841.89 points (210mm x 297mm)\n",
    "MARGIN = 2.5 * cm  # 25mm margins on all sides\n",
    "CONTENT_WIDTH = PAGE_WIDTH - 2 * MARGIN  # ~16cm usable width\n",
    "\n",
    "# Image sizing - CONSISTENT across ALL pages\n",
    "IMAGE_MAX_WIDTH = CONTENT_WIDTH  # Full content width\n",
    "IMAGE_MAX_HEIGHT = 12 * cm  # 12cm max height for consistency\n",
    "```\n",
    "\n",
    "## Helper Functions (MUST IMPLEMENT EXACTLY AS SHOWN)\n",
    "\n",
    "```python\n",
    "def resize_image_to_fit(image_path, max_width, max_height):\n",
    "    \\\"\\\"\\\"\n",
    "    Resize image maintaining aspect ratio to fit within max dimensions.\n",
    "    Centers the image horizontally.\n",
    "    \\\"\\\"\\\"\n",
    "    from reportlab.platypus import Image\n",
    "    \n",
    "    img = Image(image_path)\n",
    "    img_width = img.imageWidth\n",
    "    img_height = img.imageHeight\n",
    "    aspect = img_height / img_width\n",
    "    \n",
    "    # Calculate target dimensions\n",
    "    target_width = max_width\n",
    "    target_height = target_width * aspect\n",
    "    \n",
    "    # If too tall, scale down by height\n",
    "    if target_height > max_height:\n",
    "        target_height = max_height\n",
    "        target_width = target_height / aspect\n",
    "    \n",
    "    # Set final dimensions\n",
    "    img.drawWidth = target_width\n",
    "    img.drawHeight = target_height\n",
    "    img.hAlign = 'CENTER'  # Center horizontally\n",
    "    \n",
    "    return img\n",
    "\n",
    "def add_header_footer(canvas, doc):\n",
    "    \\\"\\\"\\\"Add header and footer to all pages except cover page\\\"\\\"\\\"\n",
    "    canvas.saveState()\n",
    "    \n",
    "    if doc.page > 1:  # Skip cover page\n",
    "        # Header\n",
    "        canvas.setFont('Helvetica', 9)\n",
    "        canvas.setFillColor(colors.grey)\n",
    "        canvas.drawString(MARGIN, PAGE_HEIGHT - 1.5*cm, \"Warpage Analysis Report\")\n",
    "        \n",
    "        # Footer with page number\n",
    "        canvas.setFont('Helvetica', 9)\n",
    "        page_num_text = f\"Page {{doc.page - 1}}\"  # Exclude cover from count\n",
    "        canvas.drawCentredString(PAGE_WIDTH / 2, 1.5*cm, page_num_text)\n",
    "    \n",
    "    canvas.restoreState()\n",
    "```\n",
    "\n",
    "## Style Definitions (USE THESE EXACT STYLES)\n",
    "\n",
    "```python\n",
    "styles = getSampleStyleSheet()\n",
    "\n",
    "# Custom styles for consistency\n",
    "style_title = ParagraphStyle(\n",
    "    'CustomTitle',\n",
    "    parent=styles['Title'],\n",
    "    fontSize=24,\n",
    "    textColor=colors.HexColor('#1f4788'),\n",
    "    alignment=TA_CENTER,\n",
    "    spaceAfter=12,\n",
    "    leading=30\n",
    ")\n",
    "\n",
    "style_subtitle = ParagraphStyle(\n",
    "    'CustomSubtitle',\n",
    "    parent=styles['Normal'],\n",
    "    fontSize=14,\n",
    "    textColor=colors.HexColor('#1f4788'),\n",
    "    alignment=TA_CENTER,\n",
    "    spaceAfter=6\n",
    ")\n",
    "\n",
    "style_heading1 = ParagraphStyle(\n",
    "    'CustomHeading1',\n",
    "    parent=styles['Heading1'],\n",
    "    fontSize=18,\n",
    "    textColor=colors.HexColor('#1f4788'),\n",
    "    spaceAfter=12,\n",
    "    spaceBefore=0\n",
    ")\n",
    "\n",
    "style_heading2 = ParagraphStyle(\n",
    "    'CustomHeading2',\n",
    "    parent=styles['Heading2'],\n",
    "    fontSize=14,\n",
    "    textColor=colors.HexColor('#1f4788'),\n",
    "    spaceAfter=8,\n",
    "    spaceBefore=0\n",
    ")\n",
    "\n",
    "style_body = ParagraphStyle(\n",
    "    'CustomBody',\n",
    "    parent=styles['BodyText'],\n",
    "    fontSize=11,\n",
    "    alignment=TA_JUSTIFY,\n",
    "    spaceAfter=8,\n",
    "    leading=14\n",
    ")\n",
    "\n",
    "style_bullet = ParagraphStyle(\n",
    "    'CustomBullet',\n",
    "    parent=styles['BodyText'],\n",
    "    fontSize=11,\n",
    "    leftIndent=20,\n",
    "    spaceAfter=6,\n",
    "    leading=14,\n",
    "    bulletIndent=10\n",
    ")\n",
    "```\n",
    "\n",
    "## Document Structure (FOLLOW THIS EXACT ORDER)\n",
    "\n",
    "```python\n",
    "# Initialize document\n",
    "filename = f'Warpage_Report_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.pdf'\n",
    "doc = SimpleDocTemplate(\n",
    "    filename,\n",
    "    pagesize=A4,\n",
    "    leftMargin=MARGIN,\n",
    "    rightMargin=MARGIN,\n",
    "    topMargin=MARGIN,\n",
    "    bottomMargin=MARGIN\n",
    ")\n",
    "\n",
    "# Story list - holds all content\n",
    "story = []\n",
    "\n",
    "# === PAGE 1: COVER PAGE ===\n",
    "story.append(Spacer(1, 6*cm))\n",
    "story.append(Paragraph(\"Automatic Warpage Analysis Report\", style_title))\n",
    "story.append(Spacer(1, 1*cm))\n",
    "story.append(Paragraph(f\"Analysis of {total_files} Measurements ({len(stats_paths)} Production Dates)\", style_subtitle))\n",
    "story.append(Spacer(1, 0.5*cm))\n",
    "story.append(Paragraph(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}\", style_body))\n",
    "story.append(PageBreak())\n",
    "\n",
    "# === PAGE 2: TABLE OF CONTENTS ===\n",
    "story.append(Paragraph(\"Table of Contents\", style_heading1))\n",
    "story.append(Spacer(1, 0.5*cm))\n",
    "\n",
    "toc_data = [\n",
    "    [\"Section\", \"Page\"],\n",
    "    [\"Executive Summary\", \"3\"],\n",
    "    [\"PCA Outlier Classification\", \"4\"],\n",
    "    [\"Bad Outlier Details\", \"5\"],\n",
    "    [\"Production Comparison\", \"6\"],\n",
    "    [\"Additional Analysis\", \"7+\"],\n",
    "    [\"Recommendations\", \"Last\"]\n",
    "]\n",
    "\n",
    "toc_table = Table(toc_data, colWidths=[CONTENT_WIDTH * 0.7, CONTENT_WIDTH * 0.3])\n",
    "toc_table.setStyle(TableStyle([\n",
    "    ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#1f4788')),\n",
    "    ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n",
    "    ('ALIGN', (0, 0), (-1, -1), 'LEFT'),\n",
    "    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n",
    "    ('FONTSIZE', (0, 0), (-1, 0), 12),\n",
    "    ('BOTTOMPADDING', (0, 0), (-1, 0), 12),\n",
    "    ('GRID', (0, 0), (-1, -1), 0.5, colors.grey),\n",
    "    ('FONTNAME', (0, 1), (-1, -1), 'Helvetica'),\n",
    "    ('FONTSIZE', (0, 1), (-1, -1), 11),\n",
    "    ('ROWBACKGROUNDS', (0, 1), (-1, -1), [colors.white, colors.HexColor('#f0f0f0')])\n",
    "]))\n",
    "story.append(toc_table)\n",
    "story.append(PageBreak())\n",
    "\n",
    "# === PAGE 3: EXECUTIVE SUMMARY ===\n",
    "story.append(Paragraph(\"Executive Summary\", style_heading1))\n",
    "story.append(Spacer(1, 0.5*cm))\n",
    "story.append(Paragraph(\"Key findings from the warpage analysis:\", style_body))\n",
    "story.append(Spacer(1, 0.3*cm))\n",
    "\n",
    "# Add bullet points summarizing Phase 1 & 2 findings\n",
    "summary_points = [\n",
    "    f\"Total measurements analyzed: {total_files}\",\n",
    "    \"PCA-based outlier detection identified critical quality issues\",\n",
    "    \"Production date comparison reveals significant quality variations\",\n",
    "    \"Bad outliers show elevated range, mean, and standard deviation\",\n",
    "]\n",
    "\n",
    "for point in summary_points:\n",
    "    story.append(Paragraph(f\"• {point}\", style_bullet))\n",
    "\n",
    "story.append(PageBreak())\n",
    "\n",
    "# === PAGE 4: PCA OUTLIER CLASSIFICATION ===\n",
    "story.append(Paragraph(\"PCA Outlier Classification\", style_heading2))\n",
    "story.append(Spacer(1, 0.3*cm))\n",
    "story.append(Paragraph(\n",
    "    \"The scatter plot below shows PC1 vs PC2 values with outliers classified into three categories: \"\n",
    "    \"normal (blue), good outliers with unusual PCA position but acceptable metrics (orange), \"\n",
    "    \"and bad outliers with critical quality issues (red).\",\n",
    "    style_body\n",
    "))\n",
    "story.append(Spacer(1, 0.5*cm))\n",
    "\n",
    "img = resize_image_to_fit(\"temp_charts/pca_outliers_classified.png\", IMAGE_MAX_WIDTH, IMAGE_MAX_HEIGHT)\n",
    "story.append(img)\n",
    "story.append(PageBreak())\n",
    "\n",
    "# === PAGE 5: BAD OUTLIER DETAILS ===\n",
    "story.append(Paragraph(\"Bad Outlier Details\", style_heading2))\n",
    "story.append(Spacer(1, 0.3*cm))\n",
    "story.append(Paragraph(\n",
    "    \"This chart compares bad outliers against average metrics, showing elevated range, \"\n",
    "    \"mean, and standard deviation values that indicate critical warpage issues.\",\n",
    "    style_body\n",
    "))\n",
    "story.append(Spacer(1, 0.5*cm))\n",
    "\n",
    "img = resize_image_to_fit(\"temp_charts/bad_outliers_detail.png\", IMAGE_MAX_WIDTH, IMAGE_MAX_HEIGHT)\n",
    "story.append(img)\n",
    "story.append(PageBreak())\n",
    "\n",
    "# === PAGE 6: PRODUCTION COMPARISON ===\n",
    "story.append(Paragraph(\"Production Comparison\", style_heading2))\n",
    "story.append(Spacer(1, 0.3*cm))\n",
    "story.append(Paragraph(\n",
    "    \"Quality comparison between production dates reveals which batch demonstrated better performance \"\n",
    "    \"and lower warpage variability.\",\n",
    "    style_body\n",
    "))\n",
    "story.append(Spacer(1, 0.5*cm))\n",
    "\n",
    "img = resize_image_to_fit(\"temp_charts/production_comparison.png\", IMAGE_MAX_WIDTH, IMAGE_MAX_HEIGHT)\n",
    "story.append(img)\n",
    "story.append(PageBreak())\n",
    "\n",
    "# === PAGES 7+: ADDITIONAL CHARTS ===\n",
    "# Find all charts in temp_charts/ directory\n",
    "import glob\n",
    "all_charts = glob.glob(\"temp_charts/*.png\")\n",
    "required_charts = [\"pca_outliers_classified.png\", \"bad_outliers_detail.png\", \"production_comparison.png\"]\n",
    "\n",
    "for chart_path in sorted(all_charts):\n",
    "    chart_filename = Path(chart_path).name\n",
    "    \n",
    "    # Skip already included charts\n",
    "    if chart_filename in required_charts:\n",
    "        continue\n",
    "    \n",
    "    # Generate title from filename\n",
    "    chart_title = chart_filename.replace(\"_\", \" \").replace(\".png\", \"\").title()\n",
    "    \n",
    "    story.append(Paragraph(chart_title, style_heading2))\n",
    "    story.append(Spacer(1, 0.3*cm))\n",
    "    story.append(Paragraph(f\"Additional analysis: {chart_title}\", style_body))\n",
    "    story.append(Spacer(1, 0.5*cm))\n",
    "    \n",
    "    img = resize_image_to_fit(chart_path, IMAGE_MAX_WIDTH, IMAGE_MAX_HEIGHT)\n",
    "    story.append(img)\n",
    "    story.append(PageBreak())\n",
    "\n",
    "# === LAST PAGE: RECOMMENDATIONS ===\n",
    "story.append(Paragraph(\"Recommendations\", style_heading1))\n",
    "story.append(Spacer(1, 0.5*cm))\n",
    "story.append(Paragraph(\"Based on the analysis, the following actions are recommended:\", style_body))\n",
    "story.append(Spacer(1, 0.3*cm))\n",
    "\n",
    "recommendations = [\n",
    "    \"Investigate root causes of bad outliers identified in PCA analysis\",\n",
    "    \"Implement stricter quality controls for production batches with higher warpage\",\n",
    "    \"Focus process improvements on reducing range variability\",\n",
    "    \"Consider real-time PCA monitoring for early outlier detection\",\n",
    "    \"Review and optimize manufacturing parameters for problematic production dates\"\n",
    "]\n",
    "\n",
    "for rec in recommendations:\n",
    "    story.append(Paragraph(f\"• {rec}\", style_bullet))\n",
    "\n",
    "# === BUILD PDF ===\n",
    "doc.build(story, onFirstPage=add_header_footer, onLaterPages=add_header_footer)\n",
    "\n",
    "print(f\"✓ PDF generated: {filename}\")\n",
    "```\n",
    "\n",
    "## Critical Requirements\n",
    "\n",
    "**MUST follow these rules:**\n",
    "1. **Portrait orientation ONLY** - Never change page orientation\n",
    "2. **Consistent image sizing** - Always use `resize_image_to_fit()` with IMAGE_MAX_WIDTH and IMAGE_MAX_HEIGHT\n",
    "3. **One section per page** - Always add `PageBreak()` after each section\n",
    "4. **Use defined styles** - Only use the custom styles defined above\n",
    "5. **Maintain aspect ratios** - Images must scale proportionally\n",
    "6. **Center images** - Use `img.hAlign = 'CENTER'`\n",
    "7. **Consistent spacing** - Use Spacer elements as shown\n",
    "\n",
    "## Expected Output\n",
    "\n",
    "Filename: `Warpage_Report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf`\n",
    "\n",
    "**Verify before completing:**\n",
    "- [ ] All images are same width and properly centered\n",
    "- [ ] All pages are portrait orientation\n",
    "- [ ] Each section starts on a new page\n",
    "- [ ] Headers/footers appear on all pages except cover1\n",
    "- [ ] TOC is properly formatted\n",
    "- [ ] Professional color scheme (#1f4788 for headings)\n",
    "\n",
    "IMPLEMENT THIS EXACTLY AS SPECIFIED!\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 3: PDF REPORT ASSEMBLY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "start = time.time()\n",
    "pdf_result, _ = client.chat_continue(\n",
    "    MODEL, session_id, pdf_prompt, agent_type=\"auto\", files=file_paths_str\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ PDF report completed in {time.time() - start:.1f}s\\n\")\n",
    "print(\"=\" * 80)\n",
    "display(Latex(pdf_result))\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
