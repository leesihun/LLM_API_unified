{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM API ‚Äì End-to-End Examples (Single Notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# !{sys.executable} -m pip install httpx\n",
    "# ! pip install pip-system-certs\n",
    "\n",
    "# API_BASE_URL = \"http://10.198.112.203:10007\"\n",
    "API_BASE_URL = 'http://localhost:10007'\n",
    "print(\"Using:\", API_BASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Iterator\n",
    "\n",
    "class LLMApiClient:\n",
    "    def __init__(self, base_url: str, timeout: float = 3600.0):\n",
    "        self.base_url = base_url.rstrip(\"/\")\n",
    "        self.token = None\n",
    "        self.timeout = httpx.Timeout(50.0, read=timeout, write=timeout, pool=timeout)\n",
    "\n",
    "    def _headers(self):\n",
    "        h = {}\n",
    "        if self.token:\n",
    "            h[\"Authorization\"] = f\"Bearer {self.token}\"\n",
    "        return h\n",
    "\n",
    "    def signup(self, username: str, password: str, role: str = \"guest\"):\n",
    "        r = httpx.post(f\"{self.base_url}/api/auth/signup\", json={\n",
    "            \"username\": username, \"password\": password, \"role\": role\n",
    "        }, timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "\n",
    "    def login(self, username: str, password: str):\n",
    "        r = httpx.post(f\"{self.base_url}/api/auth/login\", json={\n",
    "            \"username\": username, \"password\": password\n",
    "        }, timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        self.token = data[\"access_token\"]\n",
    "        return data\n",
    "\n",
    "    def list_models(self):\n",
    "        # JSON endpoints still use Content-Type header\n",
    "        headers = {\"Authorization\": f\"Bearer {self.token}\"} if self.token else {}\n",
    "        r = httpx.get(f\"{self.base_url}/v1/models\", headers=headers, timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "\n",
    "    def change_model(self, model: str):\n",
    "        headers = {\"Authorization\": f\"Bearer {self.token}\", \"Content-Type\": \"application/json\"} if self.token else {\"Content-Type\": \"application/json\"}\n",
    "        r = httpx.post(f\"{self.base_url}/api/admin/model\", json={\"model\": model}, headers=headers, timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "\n",
    "    def chat_new(self, model: str, user_message: str, agent_type: str = \"auto\", files: list = None):\n",
    "        messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "        \n",
    "        # Prepare form data\n",
    "        data = {\n",
    "            \"model\": model,\n",
    "            \"messages\": json.dumps(messages),\n",
    "            \"agent_type\": agent_type\n",
    "        }\n",
    "        \n",
    "        # Prepare files for upload\n",
    "        files_to_upload = []\n",
    "        if files:\n",
    "            for file_path in files:\n",
    "                f = open(file_path, \"rb\")\n",
    "                files_to_upload.append((\"files\", (Path(file_path).name, f)))\n",
    "        \n",
    "        try:\n",
    "            r = httpx.post(\n",
    "                f\"{self.base_url}/v1/chat/completions\",\n",
    "                data=data,\n",
    "                files=files_to_upload if files_to_upload else None,\n",
    "                headers=self._headers(),\n",
    "                timeout=self.timeout\n",
    "            )\n",
    "            r.raise_for_status()\n",
    "            result = r.json()\n",
    "            return result[\"choices\"][0][\"message\"][\"content\"], result[\"x_session_id\"]\n",
    "        \n",
    "        finally:\n",
    "            # Close file handles\n",
    "            for _, (_, f) in files_to_upload:\n",
    "                f.close()\n",
    "\n",
    "    def chat_new_streaming(self, model: str, user_message: str, agent_type: str = \"auto\", files: list = None) -> Iterator[str]:\n",
    "        \"\"\"\n",
    "        Start new chat with streaming response (Server-Sent Events)\n",
    "        \n",
    "        Args:\n",
    "            model: Model name\n",
    "            user_message: User message\n",
    "            agent_type: Agent type (auto, react, plan_execute) - Note: streaming only works for simple chat\n",
    "            files: Optional list of file paths to attach\n",
    "        \n",
    "        Yields:\n",
    "            Response tokens as they're generated\n",
    "            \n",
    "        Returns:\n",
    "            Iterator[str]: Yields tokens, then yields session_id as final value with prefix \"SESSION_ID:\"\n",
    "        \"\"\"\n",
    "        messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "        \n",
    "        # Prepare form data\n",
    "        data = {\n",
    "            \"model\": model,\n",
    "            \"messages\": json.dumps(messages),\n",
    "            \"agent_type\": agent_type,\n",
    "            \"stream\": \"true\"  # Enable streaming\n",
    "        }\n",
    "        \n",
    "        # Prepare files for upload\n",
    "        files_to_upload = []\n",
    "        if files:\n",
    "            for file_path in files:\n",
    "                f = open(file_path, \"rb\")\n",
    "                files_to_upload.append((\"files\", (Path(file_path).name, f)))\n",
    "        \n",
    "        try:\n",
    "            with httpx.stream(\n",
    "                \"POST\",\n",
    "                f\"{self.base_url}/v1/chat/completions\",\n",
    "                data=data,\n",
    "                files=files_to_upload if files_to_upload else None,\n",
    "                headers=self._headers(),\n",
    "                timeout=self.timeout\n",
    "            ) as response:\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                session_id = None\n",
    "                for line in response.iter_lines():\n",
    "                    if line.startswith(\"data: \"):\n",
    "                        data_str = line[6:]  # Remove \"data: \" prefix\n",
    "                        \n",
    "                        if data_str == \"[DONE]\":\n",
    "                            # Stream complete\n",
    "                            break\n",
    "                        \n",
    "                        try:\n",
    "                            chunk = json.loads(data_str)\n",
    "                            \n",
    "                            # Check for errors\n",
    "                            if \"error\" in chunk:\n",
    "                                raise Exception(f\"Streaming error: {chunk['error']['message']}\")\n",
    "                            \n",
    "                            # Extract session_id from final chunk\n",
    "                            if \"x_session_id\" in chunk:\n",
    "                                session_id = chunk[\"x_session_id\"]\n",
    "                            \n",
    "                            # Yield content delta\n",
    "                            if \"choices\" in chunk and len(chunk[\"choices\"]) > 0:\n",
    "                                delta = chunk[\"choices\"][0].get(\"delta\", {})\n",
    "                                if \"content\" in delta:\n",
    "                                    yield delta[\"content\"]\n",
    "                        \n",
    "                        except json.JSONDecodeError:\n",
    "                            # Skip malformed JSON\n",
    "                            continue\n",
    "                \n",
    "                # Yield session_id at the end with special prefix\n",
    "                if session_id:\n",
    "                    yield f\"SESSION_ID:{session_id}\"\n",
    "        \n",
    "        finally:\n",
    "            # Close file handles\n",
    "            for _, (_, f) in files_to_upload:\n",
    "                f.close()\n",
    "\n",
    "    def chat_continue(self, model: str, session_id: str, user_message: str, agent_type: str = \"auto\", files: list = None):\n",
    "        messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "        \n",
    "        data = {\n",
    "            \"model\": model,\n",
    "            \"messages\": json.dumps(messages),\n",
    "            \"session_id\": session_id,\n",
    "            \"agent_type\": agent_type\n",
    "        }\n",
    "        \n",
    "        files_to_upload = []\n",
    "        if files:\n",
    "            for file_path in files:\n",
    "                f = open(file_path, \"rb\")\n",
    "                files_to_upload.append((\"files\", (Path(file_path).name, f)))\n",
    "        \n",
    "        try:\n",
    "            r = httpx.post(\n",
    "                f\"{self.base_url}/v1/chat/completions\",\n",
    "                data=data,\n",
    "                files=files_to_upload if files_to_upload else None,\n",
    "                headers=self._headers(),\n",
    "                timeout=self.timeout\n",
    "            )\n",
    "            r.raise_for_status()\n",
    "            result = r.json()\n",
    "            return result[\"choices\"][0][\"message\"][\"content\"], result[\"x_session_id\"]\n",
    "        \n",
    "        finally:\n",
    "            for _, (_, f) in files_to_upload:\n",
    "                f.close()\n",
    "\n",
    "    def chat_continue_streaming(self, model: str, session_id: str, user_message: str, agent_type: str = \"auto\", files: list = None) -> Iterator[str]:\n",
    "        messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "        \n",
    "        data = {\n",
    "            \"model\": model,\n",
    "            \"messages\": json.dumps(messages),\n",
    "            \"session_id\": session_id,\n",
    "            \"agent_type\": agent_type,\n",
    "            \"stream\": \"true\"\n",
    "        }\n",
    "        \n",
    "        files_to_upload = []\n",
    "        if files:\n",
    "            for file_path in files:\n",
    "                f = open(file_path, \"rb\")\n",
    "                files_to_upload.append((\"files\", (Path(file_path).name, f)))\n",
    "        \n",
    "        try:\n",
    "            with httpx.stream(\n",
    "                \"POST\",\n",
    "                f\"{self.base_url}/v1/chat/completions\",\n",
    "                data=data,\n",
    "                files=files_to_upload if files_to_upload else None,\n",
    "                headers=self._headers(),\n",
    "                timeout=self.timeout\n",
    "            ) as response:\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                for line in response.iter_lines():\n",
    "                    if line.startswith(\"data: \"):\n",
    "                        data_str = line[6:]\n",
    "                        \n",
    "                        if data_str == \"[DONE]\":\n",
    "                            break\n",
    "                        \n",
    "                        try:\n",
    "                            chunk = json.loads(data_str)\n",
    "                            \n",
    "                            if \"error\" in chunk:\n",
    "                                raise Exception(f\"Streaming error: {chunk['error']['message']}\")\n",
    "                            \n",
    "                            if \"choices\" in chunk and len(chunk[\"choices\"]) > 0:\n",
    "                                delta = chunk[\"choices\"][0].get(\"delta\", {})\n",
    "                                if \"content\" in delta:\n",
    "                                    yield delta[\"content\"]\n",
    "                        \n",
    "                        except json.JSONDecodeError:\n",
    "                            continue\n",
    "        \n",
    "        finally:\n",
    "            for _, (_, f) in files_to_upload:\n",
    "                f.close()\n",
    "\n",
    "    def chat_sessions(self):\n",
    "        r = httpx.get(f\"{self.base_url}/api/chat/sessions\", headers=self._headers(), timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()[\"sessions\"]\n",
    "\n",
    "    def chat_history(self, session_id: str):\n",
    "        r = httpx.get(f\"{self.base_url}/api/chat/history/{session_id}\", headers=self._headers(), timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()[\"messages\"]\n",
    "\n",
    "    def tools(self):\n",
    "        r = httpx.get(f\"{self.base_url}/api/tools/list\", headers=self._headers(), timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()[\"tools\"]\n",
    "\n",
    "    def websearch(self, query: str, max_results: int = 5):\n",
    "        headers = {\"Authorization\": f\"Bearer {self.token}\", \"Content-Type\": \"application/json\"} if self.token else {\"Content-Type\": \"application/json\"}\n",
    "        # Increased timeout to 1 hour (3600s) for web search + LLM answer generation\n",
    "        r = httpx.post(f\"{self.base_url}/api/tools/websearch\", json={\"query\": query, \"max_results\": max_results}, headers=headers, timeout=3600.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()  # Returns full response with answer, results, and sources_used\n",
    "\n",
    "    def answer_from_json(self, model: str, json_blob: dict, question: str):\n",
    "        prompt = f\"Given this JSON: {json_blob}\\nAnswer: {question}\"\n",
    "        return self.chat_new(model, prompt)[0]\n",
    "\n",
    "client = LLMApiClient(API_BASE_URL, timeout=3600.0)  # 1 hour timeout\n",
    "print(\"Client ready with 3600s (1 hour) timeout for all requests\")\n",
    "print(\"‚úì Now supports multipart/form-data with optional file attachments\")\n",
    "print(\"‚úì Now supports streaming responses via chat_new_streaming() and chat_continue_streaming()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Create a new account (skip if user already exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"leesihun\"\n",
    "password = \"s.hun.lee\"\n",
    "try:\n",
    "    result = client.signup(username, password)\n",
    "    print(f\"Account created: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"Signup skipped (user may already exist): {e}\")\n",
    "    print(\"Continuing with existing account...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login = client.login(username, password)\n",
    "login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Change models (admin only) ‚Äì optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.login(\"admin\", \"administrator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = client.list_models()\n",
    "models\n",
    "\n",
    "MODEL = models[\"data\"][0][\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Start a new chat and get a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply, session_id = client.chat_new(MODEL, \"Hello! Give me a short haiku about autumn.\")\n",
    "reply, session_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Continue an existing chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply2, _ = client.chat_continue(MODEL, session_id, \"Now do one about winter.\")\n",
    "reply2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) See chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.chat_sessions(), client.chat_history(session_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Websearch with LLM-generated answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disabled because current server doens't have internet connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 7b) Websearch example - Sports news\n",
    "# # Another example showing the LLM answer generation\n",
    "# client.login(\"leesihun\", \"s.hun.lee\")\n",
    "# search_query = \"What was the latest game of Liverpool FC and who won? The current date is 2025/12/12\"\n",
    "# search_response, _ = client.chat_new(MODEL, search_query, agent_type = 'react')\n",
    "\n",
    "# print(\"=== LLM-Generated Answer ===\")\n",
    "# print(search_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) Agentic tool usage - Let the LLM decide which tool to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math_reply, _ = client.chat_continue(MODEL, session_id, \"What is 11.951/3.751?\", agent_type='react')\n",
    "print(\"Math Question Response:\")\n",
    "from IPython.display import display, Math, Latex\n",
    "display(Latex(math_reply))\n",
    "print(math_reply)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9) Sequential reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This triggers the ReAct agent because it requires step-by-step thinking\n",
    "sequential_query = \"\"\"\n",
    "First, search the web to find the latest population of Tokyo.\n",
    "Then, calculate what 15% of that population would be.\n",
    "Finally, tell me the result.\n",
    "Think hard, try to answer to best of your knowledge\n",
    "\"\"\"\n",
    "react_reply, _ = client.chat_continue(MODEL, session_id, sequential_query)\n",
    "print(\"Sequential Reasoning (ReAct) Response:\")\n",
    "\n",
    "display(Latex(react_reply))\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10) Plan-and-Execute agent with multiple tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This triggers Plan-and-Execute agent because it uses \"and\" for parallel tasks\n",
    "parallel_query = \"\"\"\n",
    "Search for the latest news about artificial intelligence and\n",
    "calculate the result of (100 * 0.15 + 25) / 2 and\n",
    "Think about what god is and\n",
    "What the best smart phone is and\n",
    "what is 1007*1007/4524753.\n",
    "\"\"\"\n",
    "plan_reply, _ = client.chat_continue(MODEL,session_id,  parallel_query, agent_type=\"plan_execute\")\n",
    "print(\"Plan-and-Execute Response:\")\n",
    "\n",
    "display(Latex(plan_reply))\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11) Auto agent selection - Let the router decide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The smart router will analyze the query and pick the best agent\n",
    "auto_query = \"If the capital of France has a population of 2.1 million, and we need to allocate 500 euros per person for a project, what's the total budget needed? First search for the actual population, then calculate.\"\n",
    "auto_reply, _ = client.chat_continue(MODEL, session_id, auto_query, agent_type=\"auto\")\n",
    "print(\"Auto Agent Selection Response:\")\n",
    "print(auto_reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Agent + RAG Documents Demo\n",
    "\n",
    "This notebook demonstrates how to **upload local documents to RAG** and use the **Auto Agent** to query RAG document collections.\n",
    "\n",
    "Key concepts:\n",
    "- Upload your own local documents (PDF, TXT, MD, DOCX, CSV, etc.) to RAG collections\n",
    "- Browse and inspect RAG collections and their documents\n",
    "- Manage collections (create, delete, remove specific documents)\n",
    "- Use the `auto` agent type which intelligently routes queries to the appropriate agent\n",
    "- When the query involves document retrieval, the auto agent selects the **ReAct agent**, which calls the **RAG tool**\n",
    "\n",
    "### How the Auto Agent Routes to RAG\n",
    "\n",
    "```\n",
    "User Query\n",
    "  -> Auto Agent (LLM decides: chat / react / plan_execute)\n",
    "    -> ReAct Agent (if tools are needed)\n",
    "      -> RAG Tool (if query involves internal documents)\n",
    "        -> Semantic Search over existing collection\n",
    "          -> LLM synthesizes answer from retrieved chunks\n",
    "```\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "1. Both servers must be running (`python tools_server.py` then `python server.py`)\n",
    "2. At least one RAG collection must already exist with uploaded documents\n",
    "3. `RAG_DEFAULT_COLLECTION` in `config.py` must match the target collection name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import json\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "class LLMApiClient:\n",
    "    \"\"\"Unified client for the LLM API server.\"\"\"\n",
    "\n",
    "    def __init__(self, base_url: str, timeout: float = 6000.0):\n",
    "        self.base_url = base_url.rstrip(\"/\")\n",
    "        self.token = None\n",
    "        self.timeout = httpx.Timeout(50.0, read=timeout, write=timeout, pool=timeout)\n",
    "\n",
    "    def _headers(self):\n",
    "        return {\"Authorization\": f\"Bearer {self.token}\"} if self.token else {}\n",
    "\n",
    "    # ---- Auth ----\n",
    "\n",
    "    def login(self, username: str, password: str):\n",
    "        r = httpx.post(\n",
    "            f\"{self.base_url}/api/auth/login\",\n",
    "            json={\"username\": username, \"password\": password},\n",
    "            timeout=10.0,\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "        self.token = r.json()[\"access_token\"]\n",
    "        return r.json()\n",
    "\n",
    "    def list_models(self):\n",
    "        r = httpx.get(f\"{self.base_url}/v1/models\", headers=self._headers(), timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "\n",
    "    # ---- Chat (auto agent) ----\n",
    "\n",
    "    def chat_new(self, model: str, user_message: str, agent_type: str = \"auto\"):\n",
    "        \"\"\"Start a new chat session. Returns (response_text, session_id).\"\"\"\n",
    "        messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "        data = {\n",
    "            \"model\": model,\n",
    "            \"messages\": json.dumps(messages),\n",
    "            \"agent_type\": agent_type,\n",
    "        }\n",
    "        r = httpx.post(\n",
    "            f\"{self.base_url}/v1/chat/completions\",\n",
    "            data=data,\n",
    "            headers=self._headers(),\n",
    "            timeout=self.timeout,\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "        result = r.json()\n",
    "        return result[\"choices\"][0][\"message\"][\"content\"], result[\"x_session_id\"]\n",
    "\n",
    "    def chat_continue(self, model: str, session_id: str, user_message: str, agent_type: str = \"auto\"):\n",
    "        \"\"\"Continue an existing session. Returns (response_text, session_id).\"\"\"\n",
    "        messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "        data = {\n",
    "            \"model\": model,\n",
    "            \"messages\": json.dumps(messages),\n",
    "            \"session_id\": session_id,\n",
    "            \"agent_type\": agent_type,\n",
    "        }\n",
    "        r = httpx.post(\n",
    "            f\"{self.base_url}/v1/chat/completions\",\n",
    "            data=data,\n",
    "            headers=self._headers(),\n",
    "            timeout=self.timeout,\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "        result = r.json()\n",
    "        return result[\"choices\"][0][\"message\"][\"content\"], result[\"x_session_id\"]\n",
    "\n",
    "    # ---- RAG management (direct tools-server calls) ----\n",
    "\n",
    "    def rag_list_collections(self, tools_base: str):\n",
    "        \"\"\"List all RAG collections for the authenticated user.\"\"\"\n",
    "        r = httpx.get(\n",
    "            f\"{tools_base}/api/tools/rag/collections\",\n",
    "            headers=self._headers(),\n",
    "            timeout=10.0,\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "\n",
    "    def rag_list_documents(self, tools_base: str, collection_name: str):\n",
    "        \"\"\"List documents in a RAG collection.\"\"\"\n",
    "        r = httpx.get(\n",
    "            f\"{tools_base}/api/tools/rag/collections/{collection_name}/documents\",\n",
    "            headers=self._headers(),\n",
    "            timeout=10.0,\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "\n",
    "    def rag_query_direct(self, tools_base: str, query: str, collection_name: str, max_results: int = 5):\n",
    "        \"\"\"Query RAG directly via the tools server (bypasses the agent).\"\"\"\n",
    "        r = httpx.post(\n",
    "            f\"{tools_base}/api/tools/rag/query\",\n",
    "            headers=self._headers(),\n",
    "            json={\n",
    "                \"query\": query,\n",
    "                \"collection_name\": collection_name,\n",
    "                \"max_results\": max_results,\n",
    "            },\n",
    "            timeout=self.timeout,\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "\n",
    "    def rag_upload_document(self, tools_base: str, collection_name: str, file_path: str):\n",
    "        \"\"\"Upload a local document to a RAG collection.\"\"\"\n",
    "        from pathlib import Path\n",
    "        \n",
    "        file_path = Path(file_path)\n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "        \n",
    "        with open(file_path, \"rb\") as f:\n",
    "            files = {\"file\": (file_path.name, f, \"application/octet-stream\")}\n",
    "            data = {\"collection_name\": collection_name}\n",
    "            \n",
    "            r = httpx.post(\n",
    "                f\"{tools_base}/api/tools/rag/upload\",\n",
    "                headers=self._headers(),\n",
    "                files=files,\n",
    "                data=data,\n",
    "                timeout=self.timeout,\n",
    "            )\n",
    "        \n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "\n",
    "    def rag_create_collection(self, tools_base: str, collection_name: str):\n",
    "        \"\"\"Create a new RAG collection.\"\"\"\n",
    "        r = httpx.post(\n",
    "            f\"{tools_base}/api/tools/rag/collections\",\n",
    "            headers=self._headers(),\n",
    "            json={\"collection_name\": collection_name},\n",
    "            timeout=10.0,\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "\n",
    "    def rag_delete_collection(self, tools_base: str, collection_name: str):\n",
    "        \"\"\"Delete a RAG collection.\"\"\"\n",
    "        r = httpx.delete(\n",
    "            f\"{tools_base}/api/tools/rag/collections/{collection_name}\",\n",
    "            headers=self._headers(),\n",
    "            timeout=10.0,\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "\n",
    "    def rag_delete_document(self, tools_base: str, collection_name: str, document_id: str):\n",
    "        \"\"\"Delete a specific document from a collection.\"\"\"\n",
    "        r = httpx.delete(\n",
    "            f\"{tools_base}/api/tools/rag/collections/{collection_name}/documents/{document_id}\",\n",
    "            headers=self._headers(),\n",
    "            timeout=10.0,\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Configuration  (adjust to your environment)\n",
    "# -------------------------------------------------------------------\n",
    "API_BASE_URL   = \"http://localhost:10007\"   # Main API server\n",
    "TOOLS_BASE_URL = \"http://localhost:10007\"   # Tools API server\n",
    "USERNAME = \"admin\"\n",
    "PASSWORD = \"administrator\"\n",
    "\n",
    "client = LLMApiClient(API_BASE_URL, timeout=6000.0)\n",
    "print(\"\\u2713 Client initialized\")\n",
    "print(f\"  Main server : {API_BASE_URL}\")\n",
    "print(f\"  Tools server: {TOOLS_BASE_URL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1: Authenticate and Discover Model\n",
    "client.login(USERNAME, PASSWORD)\n",
    "models = client.list_models()\n",
    "MODEL = models[\"data\"][0][\"id\"]\n",
    "\n",
    "print(f\"\\u2713 Logged in as: {USERNAME}\")\n",
    "print(f\"\\u2713 Using model : {MODEL}\")\n",
    "# ## Step 2: Upload Local Documents to RAG (Optional)\n",
    "\n",
    "# If you want to add your own documents to RAG, use this section. Skip to Step 3 if you already have documents uploaded.\n",
    "\n",
    "# ### Supported File Formats\n",
    "\n",
    "# - **Text**: `.txt`, `.md`\n",
    "# - **Documents**: `.pdf`, `.docx`\n",
    "# - **Data**: `.json`, `.csv`, `.xlsx`, `.xls`\n",
    "# # Create a new RAG collection and upload documents\n",
    "from pathlib import Path\n",
    "\n",
    "# Step 1: Create collection\n",
    "collection_name = \"default\"  # Change this to your desired collection name\n",
    "\n",
    "try:\n",
    "    print(f\"Creating collection '{collection_name}'...\")\n",
    "    result = client.rag_create_collection(TOOLS_BASE_URL, collection_name)\n",
    "    \n",
    "    if result.get(\"success\"):\n",
    "        print(f\"‚úì Collection created successfully!\")\n",
    "        print(f\"  Collection name: {collection_name}\\n\")\n",
    "    else:\n",
    "        print(f\"‚úó Failed to create collection: {result.get('error')}\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error creating collection: {e}\\n\")\n",
    "\n",
    "# Step 2: Upload your PDF files\n",
    "# Replace with your actual PDF file paths\n",
    "custom_pdf_files = [\n",
    "    \"./USB 3.2 Revision 1.0.pdf\",\n",
    "    \"./usb_20.pdf\"\n",
    "]\n",
    "\n",
    "print(f\"Uploading {len(custom_pdf_files)} documents...\\n\")\n",
    "\n",
    "for pdf_file in custom_pdf_files:\n",
    "    # Check if file exists\n",
    "    if not Path(pdf_file).exists():\n",
    "        print(f\"‚ö†Ô∏è  File not found: {pdf_file}\\n\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"üì§ Uploading: {pdf_file}\")\n",
    "    \n",
    "    try:\n",
    "        result = client.rag_upload_document(TOOLS_BASE_URL, collection_name, pdf_file)\n",
    "        \n",
    "        if result.get('success'):\n",
    "            print(f\"  ‚úì Success! Chunks created: {result.get('chunks_created')}\")\n",
    "            print(f\"  Total chunks in collection: {result.get('total_chunks')}\\n\")\n",
    "        else:\n",
    "            print(f\"  ‚úó Failed: {result.get('error')}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Error: {str(e)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Browse Existing RAG Collections\n",
    "\n",
    "Let's see which collections and documents are available after any uploads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collections_result = client.rag_list_collections(TOOLS_BASE_URL)\n",
    "\n",
    "if collections_result.get(\"success\"):\n",
    "    collections = collections_result[\"collections\"]\n",
    "    print(f\"Found {len(collections)} collection(s):\\n\")\n",
    "    for coll in collections:\n",
    "        print(f\"  Collection : {coll['name']}\")\n",
    "        print(f\"  Documents  : {coll['documents']}\")\n",
    "        print(f\"  Chunks     : {coll['chunks']}\")\n",
    "        print(f\"  Created    : {coll['created_at']}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"ERROR: Could not list collections.\")\n",
    "    print(collections_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize comparison utilities\n",
    "from IPython.display import HTML, Markdown\n",
    "import time\n",
    "\n",
    "def compare_responses(query, collection_name, model, session_id=None, use_markdown=False):\n",
    "    \"\"\"Execute query on both RAG and Auto Agent, display side-by-side comparison\"\"\"\n",
    "    \n",
    "    print(f\"Query: {query}\\n\")\n",
    "    print(\"=\" * 140)\n",
    "    \n",
    "    # --- Direct RAG Query ---\n",
    "    rag_start = time.time()\n",
    "    direct_result = client.rag_query_direct(\n",
    "        TOOLS_BASE_URL,\n",
    "        query=query,\n",
    "        collection_name=collection_name,\n",
    "        max_results=5,\n",
    "    )\n",
    "    rag_time = time.time() - rag_start\n",
    "    \n",
    "    # --- Auto Agent Query ---\n",
    "    agent_start = time.time()\n",
    "    if session_id is None:\n",
    "        agent_response, new_session_id = client.chat_new(\n",
    "            model=model,\n",
    "            user_message=query,\n",
    "            agent_type=\"auto\",\n",
    "        )\n",
    "    else:\n",
    "        agent_response, new_session_id = client.chat_continue(\n",
    "            model=model,\n",
    "            session_id=session_id,\n",
    "            user_message=query,\n",
    "            agent_type=\"auto\",\n",
    "        )\n",
    "    agent_time = time.time() - agent_start\n",
    "    \n",
    "    # --- Format RAG Result ---\n",
    "    if direct_result.get(\"success\"):\n",
    "        rag_answer = direct_result['answer']\n",
    "        data = direct_result.get(\"data\", {})\n",
    "        rag_sources = []\n",
    "        for i, doc in enumerate(data.get(\"documents\", []), 1):\n",
    "            rag_sources.append(f\"  [{i}] {doc['document']} chunk {doc['chunk_index']} (score {doc.get('score', 0):.3f})\")\n",
    "    else:\n",
    "        rag_answer = f\"ERROR: {direct_result.get('error')}\"\n",
    "        rag_sources = []\n",
    "    \n",
    "    # --- Display based on format preference ---\n",
    "    if use_markdown:\n",
    "        # Markdown format\n",
    "        markdown_output = f\"\"\"\n",
    "## üîç Direct RAG Query\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "{rag_answer}\n",
    "\n",
    "**Metadata:**\n",
    "- Results: {data.get('num_results', 0) if direct_result.get(\"success\") else 0} chunks retrieved\n",
    "- Exec time: {rag_time:.2f}s\n",
    "\n",
    "**Sources:**\n",
    "{chr(10).join(rag_sources) if rag_sources else \"N/A\"}\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ñ Auto Agent Query\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "{agent_response}\n",
    "\n",
    "**Metadata:**\n",
    "- Exec time: {agent_time:.2f}s\n",
    "- Session ID: {new_session_id}\n",
    "\"\"\"\n",
    "        display(Markdown(markdown_output))\n",
    "    else:\n",
    "        # HTML format\n",
    "        rag_metadata = f\"\"\"\n",
    "        <div style='font-size: 0.9em; color: #666; margin-top: 10px;'>\n",
    "        ‚úì Results: {data.get('num_results', 0) if direct_result.get(\"success\") else 0} chunks retrieved<br>\n",
    "        ‚úì Exec time: {rag_time:.2f}s<br>\n",
    "        <strong>Sources:</strong><br>\n",
    "        \"\"\"\n",
    "        for src in rag_sources:\n",
    "            rag_metadata += f\"{src}<br>\"\n",
    "        rag_metadata += \"</div>\"\n",
    "        \n",
    "        agent_metadata = f\"\"\"\n",
    "        <div style='font-size: 0.9em; color: #666; margin-top: 10px;'>\n",
    "        ‚úì Exec time: {agent_time:.2f}s<br>\n",
    "        ‚úì Session ID: {new_session_id}\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        \n",
    "        html_output = f\"\"\"\n",
    "        <div style='display: flex; gap: 20px; margin-top: 20px;'>\n",
    "            <div style='flex: 1; border: 2px solid #4CAF50; border-radius: 8px; padding: 15px; background-color: #f9f9f9;'>\n",
    "                <h3 style='color: #4CAF50; margin-top: 0;'>üîç Direct RAG Query</h3>\n",
    "                <div style='background: white; padding: 10px; border-radius: 5px; margin-bottom: 10px;'>\n",
    "                    {rag_answer.replace(chr(10), '<br>')}\n",
    "                </div>\n",
    "                {rag_metadata}\n",
    "            </div>\n",
    "            <div style='flex: 1; border: 2px solid #2196F3; border-radius: 8px; padding: 15px; background-color: #f9f9f9;'>\n",
    "                <h3 style='color: #2196F3; margin-top: 0;'>ü§ñ Auto Agent Query</h3>\n",
    "                <div style='background: white; padding: 10px; border-radius: 5px; margin-bottom: 10px;'>\n",
    "                    {agent_response.replace(chr(10), '<br>')}\n",
    "                </div>\n",
    "                {agent_metadata}\n",
    "            </div>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        display(HTML(html_output))\n",
    "    \n",
    "    print(\"=\" * 140 + \"\\n\")\n",
    "    \n",
    "    return new_session_id\n",
    "\n",
    "# Initialize session for first query\n",
    "print(\"‚úì Comparison utility loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: USB3.2 LTSSM and RX.Detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_4 = \"USB3.2Ïùò LTSSMÏóê ÎåÄÌï¥ÏÑú ÏûêÏÑ∏Ìûà ÏÑ§Î™ÖÌï¥Ï£ºÍ≥†, ÌäπÌûà RX.DetectÏóê ÎåÄÌï¥ÏÑú ÏûêÏÑ∏Ìûà ÏÑ§Î™ÖÌï¥Ï§ò\"\n",
    "session_id = compare_responses(query_4, COLLECTION_NAME, MODEL, use_markdown=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Orchestrated Report Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import time\n",
    "from IPython.display import display, Latex\n",
    "\n",
    "# Build Universal LLM API Client\n",
    "class LLMApiClient:\n",
    "    def __init__(self, base_url: str, timeout: float = 360000.0):\n",
    "        self.base_url = base_url.rstrip(\"/\")\n",
    "        self.token = None\n",
    "        self.timeout = httpx.Timeout(50.0, read=timeout, write=timeout, pool=timeout)\n",
    "\n",
    "    def _headers(self):\n",
    "        return {\"Authorization\": f\"Bearer {self.token}\"} if self.token else {}\n",
    "\n",
    "    def login(self, username: str, password: str):\n",
    "        r = httpx.post(f\"{self.base_url}/api/auth/login\", \n",
    "                      json={\"username\": username, \"password\": password}, timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        self.token = r.json()[\"access_token\"]\n",
    "        return r.json()\n",
    "\n",
    "    def list_models(self):\n",
    "        r = httpx.get(f\"{self.base_url}/v1/models\", headers=self._headers(), timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "\n",
    "    def chat_new(self, model: str, user_message: str, agent_type: str = \"auto\", files: list = None):\n",
    "        messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "        data = {\"model\": model, \"messages\": json.dumps(messages), \"agent_type\": agent_type}\n",
    "        \n",
    "        files_to_upload = []\n",
    "        if files:\n",
    "            for file_path in files:\n",
    "                f = open(file_path, \"rb\")\n",
    "                files_to_upload.append((\"files\", (Path(file_path).name, f)))\n",
    "        \n",
    "        try:\n",
    "            r = httpx.post(f\"{self.base_url}/v1/chat/completions\", data=data,\n",
    "                          files=files_to_upload if files_to_upload else None,\n",
    "                          headers=self._headers(), timeout=self.timeout)\n",
    "            r.raise_for_status()\n",
    "            result = r.json()\n",
    "            return result[\"choices\"][0][\"message\"][\"content\"], result[\"x_session_id\"]\n",
    "        finally:\n",
    "            for _, (_, f) in files_to_upload:\n",
    "                f.close()\n",
    "\n",
    "    def chat_continue(self, model: str, session_id: str, user_message: str, \n",
    "                     agent_type: str = \"auto\", files: list = None):\n",
    "        messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "        data = {\"model\": model, \"messages\": json.dumps(messages), \n",
    "                \"session_id\": session_id, \"agent_type\": agent_type}\n",
    "        \n",
    "        files_to_upload = []\n",
    "        if files:\n",
    "            for file_path in files:\n",
    "                f = open(file_path, \"rb\")\n",
    "                files_to_upload.append((\"files\", (Path(file_path).name, f)))\n",
    "        \n",
    "        try:\n",
    "            r = httpx.post(f\"{self.base_url}/v1/chat/completions\", data=data,\n",
    "                          files=files_to_upload if files_to_upload else None,\n",
    "                          headers=self._headers(), timeout=self.timeout)\n",
    "            r.raise_for_status()\n",
    "            result = r.json()\n",
    "            return result[\"choices\"][0][\"message\"][\"content\"], result[\"x_session_id\"]\n",
    "        finally:\n",
    "            for _, (_, f) in files_to_upload:\n",
    "                f.close()\n",
    "\n",
    "    def get_session_artifacts(self, session_id: str):\n",
    "        \"\"\"Get list of files generated during the session\"\"\"\n",
    "        r = httpx.get(f\"{self.base_url}/api/chat/sessions/{session_id}/artifacts\",\n",
    "                     headers=self._headers(), timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "\n",
    "    def download_artifact(self, session_id: str, filename: str, save_to: str = None):\n",
    "        \"\"\"\n",
    "        Download a generated artifact file to local disk.\n",
    "        \n",
    "        Args:\n",
    "            session_id: The session ID that generated the file\n",
    "            filename: Name of the file to download (can include subdirectory, e.g., 'temp_charts/chart.png')\n",
    "            save_to: Local path to save the file (default: current directory with original filename)\n",
    "        \n",
    "        Returns:\n",
    "            str: Path to the downloaded file\n",
    "        \n",
    "        Example:\n",
    "            client.download_artifact(session_id, \"Warpage_Report_20250126.pptx\", \"./downloads/report.pptx\")\n",
    "        \"\"\"\n",
    "        r = httpx.get(\n",
    "            f\"{self.base_url}/api/chat/sessions/{session_id}/artifacts/{filename}\",\n",
    "            headers=self._headers(),\n",
    "            timeout=60.0\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "        \n",
    "        # Determine local save path\n",
    "        if save_to is None:\n",
    "            save_to = Path(filename).name  # Use just the filename, not subdirectory\n",
    "        \n",
    "        # Create parent directories if needed\n",
    "        save_path = Path(save_to)\n",
    "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Write file content\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "        \n",
    "        return str(save_path)\n",
    "\n",
    "# Configuration\n",
    "API_BASE_URL = 'http://10.198.112.203:10007'\n",
    "USERNAME = \"ppt\"\n",
    "PASSWORD = \"ppt\"\n",
    "\n",
    "# Initialize and login\n",
    "client = LLMApiClient(API_BASE_URL, timeout=36000.0)# 10 hours\n",
    "client.login(USERNAME, PASSWORD)\n",
    "models = client.list_models()\n",
    "MODEL = models[\"data\"][0][\"id\"]\n",
    "\n",
    "print(f\"‚úì Logged in as: {USERNAME}\")\n",
    "print(f\"‚úì Using model: {MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your data files\n",
    "stats_paths = [\n",
    "    Path(\"B8_1021_stats.json\"),\n",
    "    Path(\"B8_1027_stats.json\"),\n",
    "]\n",
    "\n",
    "# Verify files exist\n",
    "print(f\"Configured {len(stats_paths)} data file(s):\\n\")\n",
    "for i, path in enumerate(stats_paths, 1):\n",
    "    if path.exists():\n",
    "        size_kb = path.stat().st_size / 1024\n",
    "        print(f\"  [{i}] {path.name} ({size_kb:.1f} KB) - ‚úì\")\n",
    "    else:\n",
    "        print(f\"  [{i}] {path.name} - ‚úó NOT FOUND\")\n",
    "\n",
    "file_paths_str = [str(p) for p in stats_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Phase 1: Data Analysis\n",
    "\n",
    "The AI will analyze your data and identify key patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_prompt = f\"\"\"\n",
    "Analyze {len(stats_paths)} warpage measurement JSON files attached.\n",
    "\n",
    "Input Data Structure:\n",
    "- Each file contain warpage statistics per PCB board\n",
    "- Statistics: min, max, range (warpage value), mean, median, std, skewness, kurtosis\n",
    "- PCA values (pc1, pc2) calculated within each source_pdf\n",
    "- Filenames contain acquisition date/time (e.g., 1021 = October 21th)\n",
    "- Note that usually, mean, median is not important. To assess warpage, range is the single most important feature.\n",
    "\n",
    "Tasks:\n",
    "1. Calculate overall statistics (mean, std, min, max of range across all files)\n",
    "2. Identify PCA-based outliers using pc1, pc2 values. Look for PCA values that are quite a far from others\n",
    "3. Compare production dates - which is better quality and why?\n",
    "4. List specific outlier filenames with reasons\n",
    "5. Save your results to a numpy array locally\n",
    "\n",
    "**Required Output:**\n",
    "- Total measurements count\n",
    "- Outlier list with full filenames\n",
    "- Production date comparison (winner + reason)\n",
    "- Key concerns or patterns\n",
    "\n",
    "Think HARD!\n",
    "\"\"\"\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 1: DATA ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "start = time.time()\n",
    "analysis_result, session_id = client.chat_new(\n",
    "    MODEL, analysis_prompt, agent_type=\"auto\", files=file_paths_str\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Analysis completed in {time.time() - start:.1f}s\\n\")\n",
    "print(\"=\" * 80)\n",
    "display(Latex(analysis_result))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Phase 2: Generate Visualizations\n",
    "\n",
    "**Key:** AI reuses Phase 1 findings from conversation memory (not raw files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_prompt = f\"\"\"\n",
    "**PRIORITY: Use your Phase 1 analysis from conversation memory and saved numpy array.**\n",
    "\n",
    "In Phase 1, you already:\n",
    "- Analyzed {len(stats_paths)} datasets and loaded all data\n",
    "- Identified PCA outliers with pc1, pc2 values\n",
    "- Compared production dates\n",
    "- Listed specific outlier filenames\n",
    "\n",
    "**Avoid re-analyze raw files if possible. Use your Phase 1 findings and file.**\n",
    "Files attached are ONLY for verification if needed.\n",
    "\n",
    "**Task:** Create visualizations and classify outliers\n",
    "\n",
    "**Outlier Classification:**\n",
    "- **BAD outliers:** High mean/std/range (critical quality issues)\n",
    "- **GOOD outliers:** Unusual PCA position but acceptable metrics\n",
    "- **Normal:** Within PCA cluster, standard metrics\n",
    "\n",
    "**Required Charts** (save to temp_charts/):\n",
    "1. `pca_outliers_classified.png` - PC1 vs PC2 scatter (Blue=normal, Orange=good outlier, RED=bad outlier)\n",
    "2. `bad_outliers_detail.png` - Bar chart comparing bad outliers vs average\n",
    "3. `production_comparison.png` - Production date quality comparison\n",
    "4. Additional charts as appropriate (distributions, trends, control charts, etc.)\n",
    "\n",
    "**Style:** 300 DPI, seaborn whitegrid, professional colors\n",
    "\n",
    "**Required Output:**\n",
    "- List of generated chart files\n",
    "- Bad outlier summary (file IDs + reasons)\n",
    "- Production date insights\n",
    "\n",
    "THINK HARD!\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 2: VISUALIZATION GENERATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "start = time.time()\n",
    "viz_result, _ = client.chat_continue(\n",
    "    MODEL, session_id, viz_prompt, agent_type=\"auto\", files=file_paths_str\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Visualizations completed in {time.time() - start:.1f}s\\n\")\n",
    "print(\"=\" * 80)\n",
    "display(Latex(viz_result))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Phase 3: PDF Report Assembly\n",
    "\n",
    "**Key:** AI uses Phase 1 & 2 findings from conversation memory to create a beautiful, comprehensive PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total file count\n",
    "total_files = 0\n",
    "for path in stats_paths:\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        total_files += len(data.get('files', []))\n",
    "\n",
    "pdf_prompt = f\"\"\"\n",
    "**Task:** Generate a professional PDF report using ReportLab with the warpage analysis findings.\n",
    "\n",
    "**Key Requirements:**\n",
    "- Use A4 portrait orientation with 2.5cm margins\n",
    "- Include: cover page, table of contents, executive summary, PCA scatter plot, bad outliers detail chart, production comparison chart, any additional charts from temp_charts/\n",
    "- Consistent styling: blue headings (#1f4788), justified body text\n",
    "- One section per page with PageBreak() between sections\n",
    "- Center all images, maintaining aspect ratio (max 12cm height)\n",
    "- Add header/footer on all pages except cover (page numbers)\n",
    "\n",
    "**Structure:**\n",
    "1. Cover page with title and total measurements\n",
    "2. Table of contents\n",
    "3. Executive summary with bullet points\n",
    "4. PCA outlier classification (with image)\n",
    "5. Bad outlier details (with image)\n",
    "6. Production comparison (with image)\n",
    "7. Additional charts from temp_charts/ directory\n",
    "8. Recommendations page\n",
    "\n",
    "**Output:** PDF file named `Warpage_Report_{{timestamp}}.pdf`\n",
    "\n",
    "Use the attached files for visualization and context.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 3: PDF REPORT ASSEMBLY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "start = time.time()\n",
    "pdf_result, _ = client.chat_continue(\n",
    "    MODEL, session_id, pdf_prompt, agent_type=\"auto\", files=file_paths_str\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì PDF report completed in {time.time() - start:.1f}s\\n\")\n",
    "print(\"=\" * 80)\n",
    "display(Latex(pdf_result))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ML CODE DIRECTORY (expanduser handles ~ expansion)\n",
    "mother_dir = os.path.expanduser('~/scratch1/MeshGraphNets')\n",
    "# docs directory\n",
    "docs_path = os.path.join(mother_dir, 'CONFIG_AND_EXECUTION_GUIDE.md')\n",
    "\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Write a python code that builds various configs for MeshGraphNets\n",
    "The docs are located at {docs_path}, So first read the docs and then write the code.\n",
    "First, think of various hyperparameters to tune.\n",
    "Then, make a set of those hyperparameters.\n",
    "Using the python code, make various config files using those hyperparameters.\n",
    "Be aware that GPU time is limited, so don't make too many configs, choose your hyperparameters wisely.\n",
    "\n",
    "When you are done, create a python script that copies the created config files to the {mother_dir} directory.\n",
    "Change the names of the config files to the right ones and run the ML code.\n",
    "\"\"\"\n",
    "\n",
    "# Send with the docs file attached\n",
    "response, session_id = client.chat_new(\n",
    "    model=MODEL,\n",
    "    user_message=prompt\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ML CODE DIRECTORY (expanduser handles ~ expansion)\n",
    "mother_dir = os.path.expanduser('~/scratch1/MeshGraphNets')\n",
    "# docs directory\n",
    "docs_path = os.path.join(mother_dir, 'CONFIG_AND_EXECUTION_GUIDE.md')\n",
    "\n",
    "# Now make the LLM API read the docs and run the code\n",
    "prompt = f\"\"\"\n",
    "You are executing a long-running hyperparameter tuning experiment for ML models.\n",
    "\n",
    "## Your Task\n",
    "1. Read the attached documentation file carefully: use python coder.\n",
    "2. Understand the configuration options and execution workflow\n",
    "3. Plan out the hyperparameter tuning experiment\n",
    "4. Build an example config file \n",
    "5. Execute the example config file with ML code.\n",
    "\n",
    "## Important Requirements\n",
    "- **Working Directory**: All code should run from `{mother_dir}`\n",
    "- **Logging**: Use the log file implemented in the code and with timestamps, distinguish filenames with config\n",
    "- **Error Handling**: If a single hyperparameter combination fails, log the error and continue with the next combination\n",
    "- **Results**: Save final results to a CSV/JSON file with all hyperparameter combinations and their metrics\n",
    "\n",
    "## Execution Guidelines\n",
    "- Use `cd {mother_dir}` at the start of your Python code\n",
    "- Do NOT stop until all hyperparameter combinations are tested\n",
    "- At the end, provide a summary of the best hyperparameters found\n",
    "\n",
    "Begin by reading the documentation, then execute the training.\n",
    "Be sure to actually execute the code, not just plan.\n",
    "\"\"\"\n",
    "\n",
    "# Send with the docs file attached\n",
    "response, session_id = client.chat_new(\n",
    "    model=MODEL,\n",
    "    user_message=prompt,\n",
    "    files=[docs_path]\n",
    ")\n",
    "\n",
    "# Save session for recovery (important for multi-day runs)\n",
    "with open(\"active_session.txt\", \"w\") as f:\n",
    "    f.write(session_id)\n",
    "print(f\"Session ID saved: {session_id}\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
