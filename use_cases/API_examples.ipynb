{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM API – End-to-End Examples (Single Notebook)\n",
    "\n",
    "This notebook shows a minimal client and step-by-step examples for:\n",
    "\n",
    "1. Create a new account\n",
    "2. Login\n",
    "3. Change models (admin privilege example)\n",
    "4. Start a new chat and get a response\n",
    "5. Continue a chat\n",
    "6. See chat history\n",
    "7. Websearch with agentic tool selection\n",
    "8. Agentic math calculation (LLM decides to use math tool)\n",
    "9. Sequential reasoning with ReAct agent (step-by-step thinking)\n",
    "10. Plan-and-Execute agent (parallel tool usage)\n",
    "11. Auto agent selection (smart router picks best agent)\n",
    "12. Complex JSON data analysis\n",
    "13. Real Data Analysis - Warpage Statistics (using 20251013_stats.json)\n",
    "14. Python Code Generation - Simple Calculation\n",
    "15. Python Code Generation - Data Analysis\n",
    "16. Python Code Generation - Mathematical Computation\n",
    "17. Python Code Generation - String Processing\n",
    "18. Python Code Generation - Excel File Analysis (Real File)\n",
    "\n",
    "Set your API base URL below if different from the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# !{sys.executable} -m pip install httpx\n",
    "# ! pip install pip-system-certs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: http://localhost:10007\n"
     ]
    }
   ],
   "source": [
    "# API_BASE_URL = \"http://10.252.38.241:10007\"\n",
    "API_BASE_URL = 'http://localhost:10007'\n",
    "print(\"Using:\", API_BASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client ready with 3600s (1 hour) timeout for all requests\n",
      "✓ Now supports multipart/form-data with optional file attachments\n",
      "✓ Now supports streaming responses via chat_new_streaming() and chat_continue_streaming()\n"
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Iterator\n",
    "\n",
    "class LLMApiClient:\n",
    "    def __init__(self, base_url: str, timeout: float = 3600.0):\n",
    "        self.base_url = base_url.rstrip(\"/\")\n",
    "        self.token = None\n",
    "        # Create timeout config: 10s for connect, custom timeout for read/write/pool\n",
    "        self.timeout = httpx.Timeout(50.0, read=timeout, write=timeout, pool=timeout)\n",
    "\n",
    "    def _headers(self):\n",
    "        # Don't set Content-Type - httpx auto-sets for multipart\n",
    "        h = {}\n",
    "        if self.token:\n",
    "            h[\"Authorization\"] = f\"Bearer {self.token}\"\n",
    "        return h\n",
    "\n",
    "    def signup(self, username: str, password: str, role: str = \"guest\"):\n",
    "        r = httpx.post(f\"{self.base_url}/api/auth/signup\", json={\n",
    "            \"username\": username, \"password\": password, \"role\": role\n",
    "        }, timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "\n",
    "    def login(self, username: str, password: str):\n",
    "        r = httpx.post(f\"{self.base_url}/api/auth/login\", json={\n",
    "            \"username\": username, \"password\": password\n",
    "        }, timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        self.token = data[\"access_token\"]\n",
    "        return data\n",
    "\n",
    "    def list_models(self):\n",
    "        # JSON endpoints still use Content-Type header\n",
    "        headers = {\"Authorization\": f\"Bearer {self.token}\"} if self.token else {}\n",
    "        r = httpx.get(f\"{self.base_url}/v1/models\", headers=headers, timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "\n",
    "    def change_model(self, model: str):\n",
    "        headers = {\"Authorization\": f\"Bearer {self.token}\", \"Content-Type\": \"application/json\"} if self.token else {\"Content-Type\": \"application/json\"}\n",
    "        r = httpx.post(f\"{self.base_url}/api/admin/model\", json={\"model\": model}, headers=headers, timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "\n",
    "    def chat_new(self, model: str, user_message: str, agent_type: str = \"auto\", files: list = None):\n",
    "        messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "        \n",
    "        # Prepare form data\n",
    "        data = {\n",
    "            \"model\": model,\n",
    "            \"messages\": json.dumps(messages),\n",
    "            \"agent_type\": agent_type\n",
    "        }\n",
    "        \n",
    "        # Prepare files for upload\n",
    "        files_to_upload = []\n",
    "        if files:\n",
    "            for file_path in files:\n",
    "                f = open(file_path, \"rb\")\n",
    "                files_to_upload.append((\"files\", (Path(file_path).name, f)))\n",
    "        \n",
    "        try:\n",
    "            r = httpx.post(\n",
    "                f\"{self.base_url}/v1/chat/completions\",\n",
    "                data=data,\n",
    "                files=files_to_upload if files_to_upload else None,\n",
    "                headers=self._headers(),\n",
    "                timeout=self.timeout\n",
    "            )\n",
    "            r.raise_for_status()\n",
    "            result = r.json()\n",
    "            return result[\"choices\"][0][\"message\"][\"content\"], result[\"x_session_id\"]\n",
    "        \n",
    "        finally:\n",
    "            # Close file handles\n",
    "            for _, (_, f) in files_to_upload:\n",
    "                f.close()\n",
    "\n",
    "    def chat_new_streaming(self, model: str, user_message: str, agent_type: str = \"auto\", files: list = None) -> Iterator[str]:\n",
    "        \"\"\"\n",
    "        Start new chat with streaming response (Server-Sent Events)\n",
    "        \n",
    "        Args:\n",
    "            model: Model name\n",
    "            user_message: User message\n",
    "            agent_type: Agent type (auto, react, plan_execute) - Note: streaming only works for simple chat\n",
    "            files: Optional list of file paths to attach\n",
    "        \n",
    "        Yields:\n",
    "            Response tokens as they're generated\n",
    "            \n",
    "        Returns:\n",
    "            Iterator[str]: Yields tokens, then yields session_id as final value with prefix \"SESSION_ID:\"\n",
    "        \"\"\"\n",
    "        messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "        \n",
    "        # Prepare form data\n",
    "        data = {\n",
    "            \"model\": model,\n",
    "            \"messages\": json.dumps(messages),\n",
    "            \"agent_type\": agent_type,\n",
    "            \"stream\": \"true\"  # Enable streaming\n",
    "        }\n",
    "        \n",
    "        # Prepare files for upload\n",
    "        files_to_upload = []\n",
    "        if files:\n",
    "            for file_path in files:\n",
    "                f = open(file_path, \"rb\")\n",
    "                files_to_upload.append((\"files\", (Path(file_path).name, f)))\n",
    "        \n",
    "        try:\n",
    "            with httpx.stream(\n",
    "                \"POST\",\n",
    "                f\"{self.base_url}/v1/chat/completions\",\n",
    "                data=data,\n",
    "                files=files_to_upload if files_to_upload else None,\n",
    "                headers=self._headers(),\n",
    "                timeout=self.timeout\n",
    "            ) as response:\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                session_id = None\n",
    "                for line in response.iter_lines():\n",
    "                    if line.startswith(\"data: \"):\n",
    "                        data_str = line[6:]  # Remove \"data: \" prefix\n",
    "                        \n",
    "                        if data_str == \"[DONE]\":\n",
    "                            # Stream complete\n",
    "                            break\n",
    "                        \n",
    "                        try:\n",
    "                            chunk = json.loads(data_str)\n",
    "                            \n",
    "                            # Check for errors\n",
    "                            if \"error\" in chunk:\n",
    "                                raise Exception(f\"Streaming error: {chunk['error']['message']}\")\n",
    "                            \n",
    "                            # Extract session_id from final chunk\n",
    "                            if \"x_session_id\" in chunk:\n",
    "                                session_id = chunk[\"x_session_id\"]\n",
    "                            \n",
    "                            # Yield content delta\n",
    "                            if \"choices\" in chunk and len(chunk[\"choices\"]) > 0:\n",
    "                                delta = chunk[\"choices\"][0].get(\"delta\", {})\n",
    "                                if \"content\" in delta:\n",
    "                                    yield delta[\"content\"]\n",
    "                        \n",
    "                        except json.JSONDecodeError:\n",
    "                            # Skip malformed JSON\n",
    "                            continue\n",
    "                \n",
    "                # Yield session_id at the end with special prefix\n",
    "                if session_id:\n",
    "                    yield f\"SESSION_ID:{session_id}\"\n",
    "        \n",
    "        finally:\n",
    "            # Close file handles\n",
    "            for _, (_, f) in files_to_upload:\n",
    "                f.close()\n",
    "\n",
    "    def chat_continue(self, model: str, session_id: str, user_message: str, agent_type: str = \"auto\", files: list = None):\n",
    "        \"\"\"\n",
    "        Continue existing chat with optional file attachments (multipart/form-data)\n",
    "        \n",
    "        Args:\n",
    "            model: Model name\n",
    "            session_id: Session ID to continue\n",
    "            user_message: User message\n",
    "            agent_type: Agent type (auto, react, plan_execute)\n",
    "            files: Optional list of file paths to attach\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (response_text, session_id)\n",
    "        \"\"\"\n",
    "        messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "        \n",
    "        data = {\n",
    "            \"model\": model,\n",
    "            \"messages\": json.dumps(messages),\n",
    "            \"session_id\": session_id,\n",
    "            \"agent_type\": agent_type\n",
    "        }\n",
    "        \n",
    "        files_to_upload = []\n",
    "        if files:\n",
    "            for file_path in files:\n",
    "                f = open(file_path, \"rb\")\n",
    "                files_to_upload.append((\"files\", (Path(file_path).name, f)))\n",
    "        \n",
    "        try:\n",
    "            r = httpx.post(\n",
    "                f\"{self.base_url}/v1/chat/completions\",\n",
    "                data=data,\n",
    "                files=files_to_upload if files_to_upload else None,\n",
    "                headers=self._headers(),\n",
    "                timeout=self.timeout\n",
    "            )\n",
    "            r.raise_for_status()\n",
    "            result = r.json()\n",
    "            return result[\"choices\"][0][\"message\"][\"content\"], result[\"x_session_id\"]\n",
    "        \n",
    "        finally:\n",
    "            for _, (_, f) in files_to_upload:\n",
    "                f.close()\n",
    "\n",
    "    def chat_continue_streaming(self, model: str, session_id: str, user_message: str, agent_type: str = \"auto\", files: list = None) -> Iterator[str]:\n",
    "        \"\"\"\n",
    "        Continue existing chat with streaming response\n",
    "        \n",
    "        Args:\n",
    "            model: Model name\n",
    "            session_id: Session ID to continue\n",
    "            user_message: User message\n",
    "            agent_type: Agent type (auto, react, plan_execute) - Note: streaming only works for simple chat\n",
    "            files: Optional list of file paths to attach\n",
    "        \n",
    "        Yields:\n",
    "            Response tokens as they're generated\n",
    "        \"\"\"\n",
    "        messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "        \n",
    "        data = {\n",
    "            \"model\": model,\n",
    "            \"messages\": json.dumps(messages),\n",
    "            \"session_id\": session_id,\n",
    "            \"agent_type\": agent_type,\n",
    "            \"stream\": \"true\"\n",
    "        }\n",
    "        \n",
    "        files_to_upload = []\n",
    "        if files:\n",
    "            for file_path in files:\n",
    "                f = open(file_path, \"rb\")\n",
    "                files_to_upload.append((\"files\", (Path(file_path).name, f)))\n",
    "        \n",
    "        try:\n",
    "            with httpx.stream(\n",
    "                \"POST\",\n",
    "                f\"{self.base_url}/v1/chat/completions\",\n",
    "                data=data,\n",
    "                files=files_to_upload if files_to_upload else None,\n",
    "                headers=self._headers(),\n",
    "                timeout=self.timeout\n",
    "            ) as response:\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                for line in response.iter_lines():\n",
    "                    if line.startswith(\"data: \"):\n",
    "                        data_str = line[6:]\n",
    "                        \n",
    "                        if data_str == \"[DONE]\":\n",
    "                            break\n",
    "                        \n",
    "                        try:\n",
    "                            chunk = json.loads(data_str)\n",
    "                            \n",
    "                            if \"error\" in chunk:\n",
    "                                raise Exception(f\"Streaming error: {chunk['error']['message']}\")\n",
    "                            \n",
    "                            if \"choices\" in chunk and len(chunk[\"choices\"]) > 0:\n",
    "                                delta = chunk[\"choices\"][0].get(\"delta\", {})\n",
    "                                if \"content\" in delta:\n",
    "                                    yield delta[\"content\"]\n",
    "                        \n",
    "                        except json.JSONDecodeError:\n",
    "                            continue\n",
    "        \n",
    "        finally:\n",
    "            for _, (_, f) in files_to_upload:\n",
    "                f.close()\n",
    "\n",
    "    def chat_sessions(self):\n",
    "        r = httpx.get(f\"{self.base_url}/api/chat/sessions\", headers=self._headers(), timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()[\"sessions\"]\n",
    "\n",
    "    def chat_history(self, session_id: str):\n",
    "        r = httpx.get(f\"{self.base_url}/api/chat/history/{session_id}\", headers=self._headers(), timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()[\"messages\"]\n",
    "\n",
    "    def tools(self):\n",
    "        r = httpx.get(f\"{self.base_url}/api/tools/list\", headers=self._headers(), timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()[\"tools\"]\n",
    "\n",
    "    def websearch(self, query: str, max_results: int = 5):\n",
    "        \"\"\"\n",
    "        Perform web search and get LLM-generated answer\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with:\n",
    "            - answer: LLM-generated answer from search results\n",
    "            - results: Raw search results (list of dicts with title, url, content, score)\n",
    "            - sources_used: List of URLs used as sources\n",
    "        \"\"\"\n",
    "        headers = {\"Authorization\": f\"Bearer {self.token}\", \"Content-Type\": \"application/json\"} if self.token else {\"Content-Type\": \"application/json\"}\n",
    "        # Increased timeout to 1 hour (3600s) for web search + LLM answer generation\n",
    "        r = httpx.post(f\"{self.base_url}/api/tools/websearch\", json={\"query\": query, \"max_results\": max_results}, headers=headers, timeout=3600.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()  # Returns full response with answer, results, and sources_used\n",
    "\n",
    "    def answer_from_json(self, model: str, json_blob: dict, question: str):\n",
    "        prompt = f\"Given this JSON: {json_blob}\\nAnswer: {question}\"\n",
    "        return self.chat_new(model, prompt)[0]\n",
    "\n",
    "client = LLMApiClient(API_BASE_URL, timeout=3600.0)  # 1 hour timeout\n",
    "print(\"Client ready with 3600s (1 hour) timeout for all requests\")\n",
    "print(\"✓ Now supports multipart/form-data with optional file attachments\")\n",
    "print(\"✓ Now supports streaming responses via chat_new_streaming() and chat_continue_streaming()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Create a new account (skip if user already exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signup skipped (user may already exist): Client error '400 Bad Request' for url 'http://localhost:10007/api/auth/signup'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400\n",
      "Continuing with existing account...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "username = \"leesihun\"\n",
    "password = \"s.hun.lee\"\n",
    "try:\n",
    "    result = client.signup(username, password)\n",
    "    print(f\"Account created: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"Signup skipped (user may already exist): {e}\")\n",
    "    print(\"Continuing with existing account...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'access_token': 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJsZWVzaWh1biIsInJvbGUiOiJ1c2VyIiwiZXhwIjoxNzcyMzY0NjI1fQ.2ksWgdW_bNBk5gq5NR0Zvt6gnoQR-bnDTbfxHBe68AY',\n",
       " 'token_type': 'bearer'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "login = client.login(username, password)\n",
    "login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Change models (admin only) – optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'access_token': 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJhZG1pbiIsInJvbGUiOiJhZG1pbiIsImV4cCI6MTc3MjM2NDYyN30.hR7xdZZjRNX_UY5uVxKlU-LimqOjWaaeYgDiSj_tYCs',\n",
       " 'token_type': 'bearer'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using deepseek-r1:1.5b for all examples\n",
    "client.login(\"admin\", \"administrator\")\n",
    "# client.change_model(\"gpt-oss:120b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List models (OpenAI-compatible)\n",
    "models = client.list_models()\n",
    "models\n",
    "\n",
    "MODEL = models[\"data\"][0][\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Start a new chat and get a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Leaves turn gold - 5\\nCool breeze whispers through the air - 7\\nWinter comes soon - 5',\n",
       " '2440cdfe-88cd-4d65-9eae-adb604e16b45')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply, session_id = client.chat_new(MODEL, \"Hello! Give me a short haiku about autumn.\")\n",
    "reply, session_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Continue an existing chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Snowflakes fall gently - 5\\nWhite blanket covers ground - 7\\nCold winds whisper now - 5'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "reply2, _ = client.chat_continue(MODEL, session_id, \"Now do one about winter.\")\n",
    "reply2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) See chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'session_id': '2440cdfe-88cd-4d65-9eae-adb604e16b45',\n",
       "   'created_at': '2026-02-22 11:30:32',\n",
       "   'message_count': 4},\n",
       "  {'session_id': 'f44c2bec-f19b-4b92-b76e-653d14942975',\n",
       "   'created_at': '2026-02-22 11:10:31',\n",
       "   'message_count': 20},\n",
       "  {'session_id': 'c05d90c4-24db-4ec5-bc7e-03332b5520d6',\n",
       "   'created_at': '2026-02-22 11:01:33',\n",
       "   'message_count': 8},\n",
       "  {'session_id': 'dfcc226b-2fdf-4b2b-92d1-fd256b199167',\n",
       "   'created_at': '2026-02-22 10:42:20',\n",
       "   'message_count': 14},\n",
       "  {'session_id': '26f06bd6-609f-4695-97fa-fdf4ba8ac128',\n",
       "   'created_at': '2026-02-22 10:28:00',\n",
       "   'message_count': 8},\n",
       "  {'session_id': 'f5b64265-6b94-4f8a-b3de-cf51075164ba',\n",
       "   'created_at': '2026-02-22 10:27:35',\n",
       "   'message_count': 2},\n",
       "  {'session_id': '80fb90e0-85ed-497b-bb11-1c9c8a7e5d46',\n",
       "   'created_at': '2025-12-11 23:40:59',\n",
       "   'message_count': 12},\n",
       "  {'session_id': '802c581b-05a5-4f0a-b3a7-b4867835ba1e',\n",
       "   'created_at': '2025-12-11 23:27:22',\n",
       "   'message_count': 2},\n",
       "  {'session_id': 'd7bffe1c-0b9c-46e7-9fbf-a71cd23112a1',\n",
       "   'created_at': '2025-12-11 22:59:07',\n",
       "   'message_count': 2},\n",
       "  {'session_id': '3cd5f353-3fff-4035-9091-4d9b3499e853',\n",
       "   'created_at': '2025-12-11 12:59:14',\n",
       "   'message_count': 4},\n",
       "  {'session_id': '0144ba9f-9eee-49b4-9bff-1a257305b85f',\n",
       "   'created_at': '2025-12-11 12:23:40',\n",
       "   'message_count': 2},\n",
       "  {'session_id': 'f1dc864a-cdc9-42b3-baf9-2ebc96bd85e5',\n",
       "   'created_at': '2025-12-11 12:14:23',\n",
       "   'message_count': 2},\n",
       "  {'session_id': '708f7143-bc2c-4dd6-8df2-96140a8e977d',\n",
       "   'created_at': '2025-12-11 12:06:56',\n",
       "   'message_count': 2},\n",
       "  {'session_id': '8bee2d32-bf64-449e-b2fb-1d2b70d3d83f',\n",
       "   'created_at': '2025-12-11 11:40:10',\n",
       "   'message_count': 4},\n",
       "  {'session_id': 'bf016b02-88ea-4450-91a4-a9e41c8537c1',\n",
       "   'created_at': '2025-12-11 11:28:10',\n",
       "   'message_count': 2},\n",
       "  {'session_id': 'c14983b9-cf8a-4a87-9d34-221351321fd8',\n",
       "   'created_at': '2025-12-11 08:56:02',\n",
       "   'message_count': 2},\n",
       "  {'session_id': '7a94bcb6-6b8d-4fbd-99a4-aaaeebeb4eb8',\n",
       "   'created_at': '2025-12-11 08:52:11',\n",
       "   'message_count': 2},\n",
       "  {'session_id': '85bda8a4-9023-4b0f-aca5-014f4d84646c',\n",
       "   'created_at': '2025-12-11 08:13:50',\n",
       "   'message_count': 8},\n",
       "  {'session_id': 'cb2abc38-90cd-4e82-981a-8f8a59b90f58',\n",
       "   'created_at': '2025-12-11 06:05:43',\n",
       "   'message_count': 6},\n",
       "  {'session_id': 'e3b5c6f6-f53f-4e27-83b5-dd0e2a2cfdb0',\n",
       "   'created_at': '2025-12-11 05:59:08',\n",
       "   'message_count': 4},\n",
       "  {'session_id': 'd727808a-3957-41e1-ac11-7078aefeae38',\n",
       "   'created_at': '2025-12-11 05:17:24',\n",
       "   'message_count': 4},\n",
       "  {'session_id': 'e44c25b3-27dc-45cc-bf12-ce06c38b5612',\n",
       "   'created_at': '2025-12-11 05:11:00',\n",
       "   'message_count': 4},\n",
       "  {'session_id': 'e893f5cf-f748-4ac0-976f-89e744729532',\n",
       "   'created_at': '2025-12-11 05:06:37',\n",
       "   'message_count': 2},\n",
       "  {'session_id': '824d2557-7768-432e-96c1-7c71d6c2fb3e',\n",
       "   'created_at': '2025-12-11 04:59:07',\n",
       "   'message_count': 2},\n",
       "  {'session_id': '1e96e805-a816-4299-af95-f41bd0c3a7db',\n",
       "   'created_at': '2025-12-11 04:55:17',\n",
       "   'message_count': 4},\n",
       "  {'session_id': 'f3b6d0a6-af1e-4763-9874-259a370c0294',\n",
       "   'created_at': '2025-12-11 04:51:56',\n",
       "   'message_count': 2},\n",
       "  {'session_id': 'd8f00bcf-89ed-47a0-8f0f-136e2e10d622',\n",
       "   'created_at': '2025-12-11 03:41:34',\n",
       "   'message_count': 4},\n",
       "  {'session_id': 'c5f6a3ba-8162-4ac0-914a-eba939656eda',\n",
       "   'created_at': '2025-12-11 02:44:39',\n",
       "   'message_count': 4},\n",
       "  {'session_id': '0c4baf85-de28-49ab-8733-ef291452ce86',\n",
       "   'created_at': '2025-12-11 02:29:08',\n",
       "   'message_count': 6},\n",
       "  {'session_id': '16ff848a-1bd4-4767-ac3e-59781f134627',\n",
       "   'created_at': '2025-12-11 02:26:11',\n",
       "   'message_count': 2},\n",
       "  {'session_id': '45025418-d26a-4e72-93cf-479f015324e4',\n",
       "   'created_at': '2025-12-11 02:24:12',\n",
       "   'message_count': 2},\n",
       "  {'session_id': 'a3419496-3363-452c-a104-42416a2439b9',\n",
       "   'created_at': '2025-12-11 02:21:41',\n",
       "   'message_count': 2},\n",
       "  {'session_id': '644039cb-5c7a-461c-b8e2-1a837a89c4af',\n",
       "   'created_at': '2025-12-11 02:03:33',\n",
       "   'message_count': 6},\n",
       "  {'session_id': '80d155f4-7db6-4a09-83db-1e76b0120780',\n",
       "   'created_at': '2025-12-10 10:36:30',\n",
       "   'message_count': 12},\n",
       "  {'session_id': 'e14ff8d8-3d1e-4224-a54e-54e7d41c1c5b',\n",
       "   'created_at': '2025-12-10 10:35:21',\n",
       "   'message_count': 6}],\n",
       " [{'role': 'user',\n",
       "   'content': 'Hello! Give me a short haiku about autumn.',\n",
       "   'tool_calls': None,\n",
       "   'tool_call_id': None,\n",
       "   'name': None},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'Leaves turn gold - 5\\nCool breeze whispers through the air - 7\\nWinter comes soon - 5',\n",
       "   'tool_calls': None,\n",
       "   'tool_call_id': None,\n",
       "   'name': None},\n",
       "  {'role': 'user',\n",
       "   'content': 'Now do one about winter.',\n",
       "   'tool_calls': None,\n",
       "   'tool_call_id': None,\n",
       "   'name': None},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'Snowflakes fall gently - 5\\nWhite blanket covers ground - 7\\nCold winds whisper now - 5',\n",
       "   'tool_calls': None,\n",
       "   'tool_call_id': None,\n",
       "   'name': None}])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "client.chat_sessions(), client.chat_history(session_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Websearch with LLM-generated answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # The API now generates an answer from search results using LLM\n",
    "# client.login(\"leesihun\", \"s.hun.lee\")\n",
    "# search_query = \"Tell me who is SiHun Lee\"\n",
    "# search_response, _ = client.chat_continue(MODEL, session_id, search_query, agent_type = 'react')\n",
    "\n",
    "# print(\"=== LLM-Generated Answer ===\")\n",
    "# print(search_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 7b) Websearch example - Sports news\n",
    "# # Another example showing the LLM answer generation\n",
    "# client.login(\"leesihun\", \"s.hun.lee\")\n",
    "# search_query = \"What was the latest game of Liverpool FC and who won? The current date is 2025/12/12\"\n",
    "# search_response, _ = client.chat_new(MODEL, search_query, agent_type = 'react')\n",
    "\n",
    "# print(\"=== LLM-Generated Answer ===\")\n",
    "# print(search_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) Agentic tool usage - Let the LLM decide which tool to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Math Question Response:\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\( \\frac{11.951}{3.751} \\approx 3.184 \\)\n",
       "\n",
       "More precisely: 3.183853939"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\( \\frac{11.951}{3.751} \\approx 3.184 \\)\n",
      "\n",
      "More precisely: 3.183853939\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "math_reply, _ = client.chat_continue(MODEL, session_id, \"What is 11.951/3.751?\", agent_type='react')\n",
    "print(\"Math Question Response:\")\n",
    "from IPython.display import display, Math, Latex\n",
    "display(Latex(math_reply))\n",
    "print(math_reply)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9) Sequential reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential Reasoning (ReAct) Response:\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "I apologize, but I wasn't able to search the web due to an API key issue. However, based on my knowledge:\n",
       "\n",
       "Tokyo's population is approximately **14 million people** (as the Tokyo metropolitan area).\n",
       "\n",
       "15% of 14,000,000 = 0.15 × 14,000,000 = **2,100,000**\n",
       "\n",
       "So 15% of Tokyo's population would be approximately **2.1 million people**.\n",
       "\n",
       "Note: If you meant the city proper population (which is around 9 million), then 15% would be about 1.35 million people."
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This triggers the ReAct agent because it requires step-by-step thinking\n",
    "sequential_query = \"\"\"\n",
    "First, search the web to find the latest population of Tokyo.\n",
    "Then, calculate what 15% of that population would be.\n",
    "Finally, tell me the result.\n",
    "Think hard, try to answer to best of your knowledge\n",
    "\"\"\"\n",
    "react_reply, _ = client.chat_continue(MODEL, session_id, sequential_query)\n",
    "print(\"Sequential Reasoning (ReAct) Response:\")\n",
    "\n",
    "display(Latex(react_reply))\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10) Plan-and-Execute agent with multiple tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plan-and-Execute Response:\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "## AI News\n",
       "⚠️ Web search unavailable due to API key error.\n",
       "\n",
       "## Calculations\n",
       "\n",
       "### (100 * 0.15 + 25) / 2 = 20.0\n",
       "\n",
       "### 1007 * 1007 / 4524753\n",
       "- 1007 × 1007 = 1,014,049\n",
       "- 1,014,049 ÷ 4,524,753 ≈ **0.2243**\n",
       "\n",
       "---\n",
       "\n",
       "## What is God?\n",
       "This is one of philosophy and theology's deepest questions. Common perspectives include:\n",
       "\n",
       "- **Theological view**: A supreme being or creator of the universe, often with attributes like omniscience, omnipotence, and omnibenevolence\n",
       "- **Philosophical view**: An ultimate reality, first cause, or ground of being—something beyond material existence\n",
       "- **Agnostic view**: The truth about God's existence is unknowable\n",
       "- **Atheist view**: No evidence supports the existence of a god\n",
       "\n",
       "---\n",
       "\n",
       "## What is the best smartphone?\n",
       "Opinions vary widely, but top contenders often include:\n",
       "\n",
       "- **iPhone 16 Pro Max** (iOS ecosystem, cameras, build quality)\n",
       "- **Samsung Galaxy S24 Ultra** (Android, S Pen, versatility)\n",
       "- **Google Pixel 9 Pro** (AI features, camera software, clean UI)\n",
       "- **OnePlus 12** (great battery, performance, value)\n",
       "\n",
       "The \"best\" depends on your priorities: camera quality, battery life, ecosystem, price, etc.\n",
       "\n",
       "---\n",
       "\n",
       "Would you like me to retry the web search, or is this sufficient?"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This triggers Plan-and-Execute agent because it uses \"and\" for parallel tasks\n",
    "parallel_query = \"\"\"\n",
    "Search for the latest news about artificial intelligence and\n",
    "calculate the result of (100 * 0.15 + 25) / 2 and\n",
    "Think about what god is and\n",
    "What the best smart phone is and\n",
    "what is 1007*1007/4524753.\n",
    "\"\"\"\n",
    "plan_reply, _ = client.chat_continue(MODEL,session_id,  parallel_query, agent_type=\"plan_execute\")\n",
    "print(\"Plan-and-Execute Response:\")\n",
    "\n",
    "display(Latex(plan_reply))\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11) Auto agent selection - Let the router decide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto Agent Selection Response:\n",
      "## Paris Population\n",
      "⚠️ Web search unavailable due to API key error.\n",
      "\n",
      "## Using 2.1 Million (based on your figure):\n",
      "**Total budget = 500 euros/person × 2,100,000 people = 1,050,000,000 euros**\n",
      "\n",
      "That's **1.05 billion euros**.\n",
      "\n",
      "## Using Actual Paris Population:\n",
      "Paris city proper has about **2.1 million** people (2019), while the metropolitan area has about **12-14 million**.\n",
      "\n",
      "- For 12 million people: 500 × 12,000,000 = **6 billion euros**\n",
      "- For 14 million people: 500 × 14,000,000 = **7 billion euros**\n",
      "\n",
      "---\n",
      "\n",
      "Would you like me to retry the web search, or is this answer sufficient?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The smart router will analyze the query and pick the best agent\n",
    "auto_query = \"If the capital of France has a population of 2.1 million, and we need to allocate 500 euros per person for a project, what's the total budget needed? First search for the actual population, then calculate.\"\n",
    "auto_reply, _ = client.chat_continue(MODEL, session_id, auto_query, agent_type=\"auto\")\n",
    "print(\"Auto Agent Selection Response:\")\n",
    "print(auto_reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12) Complex JSON data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Complex JSON Analysis Response ===\n",
      "## Analysis Results\n",
      "\n",
      "### 1. Highest Revenue Department\n",
      "| Department | Employees | Total Revenue |\n",
      "|------------|-----------|---------------|\n",
      "| Electronics | 45 | $1,139,800 |\n",
      "| Home Appliances | 32 | $298,500 |\n",
      "| Furniture | 28 | $173,400 |\n",
      "\n",
      "**Winner: Electronics** with **$1,139,800** revenue\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Average Revenue Per Employee\n",
      "- Total employees: 45 + 32 + 28 = **105**\n",
      "- Total revenue: $1,139,800 + $298,500 + $173,400 = **$1,611,700**\n",
      "- Average: $1,611,700 ÷ 105 = **$15,349.52 per employee**\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Highest Revenue Product\n",
      "| Product | Units Sold | Price | Revenue |\n",
      "|---------|------------|-------|---------|\n",
      "| Smartphone | 856 | $800 | $684,800 |\n",
      "| Laptop | 320 | $1,200 | $384,000 |\n",
      "| Tablet | 142 | $500 | $71,000 |\n",
      "| Refrigerator | 89 | $1,500 | $133,500 |\n",
      "| Washing Machine | 124 | $900 | $111,600 |\n",
      "| Desk | 178 | $450 | $80,100 |\n",
      "| Chair | 432 | $150 | $64,800 |\n",
      "| Microwave | 267 | $200 | $53,400 |\n",
      "| Bookshelf | 95 | $300 | $28,500 |\n",
      "\n",
      "**Winner: Smartphone** with **$684,800** revenue\n",
      "\n",
      "---\n",
      "\n",
      "### 4. Total Units Sold\n",
      "- Electronics: 320 + 856 + 142 = **1,318**\n",
      "- Home Appliances: 89 + 124 + 267 = **480**\n",
      "- Furniture: 178 + 432 + 95 = **705**\n",
      "\n",
      "**Grand Total: 1,318 + 480 + 705 = 2,503 units**\n",
      "\n",
      "✓ Cleaned up ./complex_json.json\n"
     ]
    }
   ],
   "source": [
    "# 12) Complex JSON data analysis (with File Upload)\n",
    "import json\n",
    "\n",
    "# Create a realistic e-commerce dataset\n",
    "complex_json = {\n",
    "    \"company\": \"TechMart Inc\",\n",
    "    \"quarter\": \"Q3 2025\",\n",
    "    \"departments\": [\n",
    "        {\n",
    "            \"name\": \"Electronics\",\n",
    "            \"employees\": 45,\n",
    "            \"sales\": [\n",
    "                {\"product\": \"Laptop\", \"units_sold\": 320, \"price\": 1200, \"revenue\": 384000},\n",
    "                {\"product\": \"Smartphone\", \"units_sold\": 856, \"price\": 800, \"revenue\": 684800},\n",
    "                {\"product\": \"Tablet\", \"units_sold\": 142, \"price\": 500, \"revenue\": 71000}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Home Appliances\",\n",
    "            \"employees\": 32,\n",
    "            \"sales\": [\n",
    "                {\"product\": \"Refrigerator\", \"units_sold\": 89, \"price\": 1500, \"revenue\": 133500},\n",
    "                {\"product\": \"Washing Machine\", \"units_sold\": 124, \"price\": 900, \"revenue\": 111600},\n",
    "                {\"product\": \"Microwave\", \"units_sold\": 267, \"price\": 200, \"revenue\": 53400}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Furniture\",\n",
    "            \"employees\": 28,\n",
    "            \"sales\": [\n",
    "                {\"product\": \"Desk\", \"units_sold\": 178, \"price\": 450, \"revenue\": 80100},\n",
    "                {\"product\": \"Chair\", \"units_sold\": 432, \"price\": 150, \"revenue\": 64800},\n",
    "                {\"product\": \"Bookshelf\", \"units_sold\": 95, \"price\": 300, \"revenue\": 28500}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save the data to a JSON file\n",
    "json_name = './complex_json.json'\n",
    "with open(json_name, 'w') as f:\n",
    "    json.dump(complex_json, f, indent=2)\n",
    "\n",
    "# Ask the LLM to analyze the attached JSON file\n",
    "analysis_query = \"\"\"\n",
    "Analyze the attached company data JSON file and tell me:\n",
    "1. Which department has the highest total revenue?\n",
    "2. What is the average revenue per employee across all departments?\n",
    "3. Which single product generated the most revenue?\n",
    "4. Calculate the total units sold across all departments.\n",
    "\n",
    "Please provide exact numbers and show your calculations.\n",
    "\"\"\"\n",
    "\n",
    "# Expected Answers:\n",
    "# 1. Electronics: 1,139,800\n",
    "# 2. 15,356.19 (1,615,300 total revenue / 105 total employees)\n",
    "# 3. Smartphone: 684,800\n",
    "# 4. 2,503 units total\n",
    "\n",
    "# NEW: Upload the JSON file instead of pasting data in prompt\n",
    "json_reply, _ = client.chat_continue(\n",
    "    MODEL, \n",
    "    session_id,\n",
    "    analysis_query,\n",
    "    files=[json_name]\n",
    ")\n",
    "\n",
    "print(\"\\n=== Complex JSON Analysis Response ===\")\n",
    "print(json_reply)\n",
    "\n",
    "# Cleanup\n",
    "Path(json_name).unlink()\n",
    "print(f\"\\n✓ Cleaned up {json_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13) Real Data Analysis - Warpage Statistics\n",
    "\n",
    "Analyze real manufacturing warpage measurement data from uploaded JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data\\\\uploads\\\\leesihun\\\\20251013_stats.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m      6\u001b[0m stats_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/uploads/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00musername\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/20251013_stats.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstats_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      8\u001b[0m     warpage_stats \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     10\u001b[0m analysis_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124mThe given file contains 50 measurements of warpage.\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124mBased on this warpage analysis data, please analyze and tell me:\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124mPlease provide specific file IDs and numeric values in your analysis.\u001b[39m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data\\\\uploads\\\\leesihun\\\\20251013_stats.json'"
     ]
    }
   ],
   "source": [
    "# 13) Real Data Analysis - Warpage Statistics from JSON\n",
    "# Load and analyze the actual warpage analysis report data\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "stats_path = Path(f\"data/uploads/{username}/20251013_stats.json\")\n",
    "with open(stats_path, 'r') as f:\n",
    "    warpage_stats = json.load(f)\n",
    "\n",
    "analysis_query = \"\"\"\n",
    "The given file contains 50 measurements of warpage.\n",
    "Based on this warpage analysis data, please analyze and tell me:\n",
    "\n",
    "1. Which has the highest maximum warpage value and what is it?\n",
    "2. Which has the lowest minimum warpage value and what is it?\n",
    "3. What is the average mean warpage across all 50 measurements?\n",
    "4. Calculate the overall standard deviation range (min std to max std) across all measurements\n",
    "5. Which measurement shows the most variability (highest range) and what is that range?\n",
    "6. What is the average kurtosis value across all measurements?\n",
    "7. Identify any files with extreme kurtosis (>47) which might indicate outliers\n",
    "\n",
    "Please provide specific file IDs and numeric values in your analysis.\n",
    "\"\"\"\n",
    "\n",
    "warpage_reply, _ = client.chat_new(MODEL, analysis_query, files=[stats_path])\n",
    "print(\"=== Warpage Statistics Analysis ===\")\n",
    "from IPython.display import display, Math, Latex\n",
    "display(Latex(warpage_reply))\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "\"\"\"\n",
    "Answer: \n",
    "\n",
    "\n",
    "1. Highest Maximum Warpage Value:\n",
    "File ID: File_48\n",
    "Filename: 20251013146640@B5913326505D7_ORI.txt\n",
    "Maximum Value: 534.0\n",
    "2. Lowest Minimum Warpage Value:\n",
    "File ID: File_04\n",
    "Filename: 20251013142156@B5913326505D7_ORI.txt\n",
    "Minimum Value: -4200.0\n",
    "3. Average Mean Warpage:\n",
    "-297.84 (across all 50 measurements)\n",
    "4. Overall Standard Deviation Range:\n",
    "Minimum Std: 71.23\n",
    "Maximum Std: 87.89\n",
    "Range: 16.66\n",
    "5. Most Variability (Highest Range):\n",
    "File ID: File_04\n",
    "Filename: 20251013142156@B5913326505D7_ORI.txt\n",
    "Range: 4723.0\n",
    "6. Average Kurtosis Value:\n",
    "44.43 (across all measurements)\n",
    "7. Files with Extreme Kurtosis (>47) - Potential Outliers: Found 7 files with extreme kurtosis:\n",
    "File ID\tFilename\tKurtosis\n",
    "File_48\t20251013146640@...\t48.35\n",
    "File_36\t20251013145428@...\t48.23\n",
    "File_24\t20251013144216@...\t48.12\n",
    "File_04\t20251013142156@...\t47.89\n",
    "File_40\t20251013145832@...\t47.57\n",
    "File_28\t20251013144620@...\t47.46\n",
    "File_12\t20251013143004@...\t47.12\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14) Python Code Generation - Simple Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Code Generation Response:\n",
      "## Results\n",
      "\n",
      "**Fibonacci sequence up to 100:**\n",
      "\n",
      "```json\n",
      "[0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89]\n",
      "```\n",
      "\n",
      "**Algorithm used:** Efficient iterative approach with O(n) time complexity and O(1) space complexity.\n",
      "\n",
      "---\n",
      "\n",
      "## Python Code Used\n",
      "\n",
      "```python\n",
      "def generate_fibonacci_up_to(limit):\n",
      "    \"\"\"Generate Fibonacci sequence up to a given limit using iterative approach.\"\"\"\n",
      "    sequence = []\n",
      "    a, b = 0, 1\n",
      "    \n",
      "    while a <= limit:\n",
      "        sequence.append(a)\n",
      "        a, b = b, a + b\n",
      "    \n",
      "    return sequence\n",
      "\n",
      "# Generate sequence up to 100\n",
      "fib_sequence = generate_fibonacci_up_to(100)\n",
      "\n",
      "# Output as JSON list\n",
      "import json\n",
      "print(json.dumps(fib_sequence))\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "**Key details:**\n",
      "- Starts with 0, 1\n",
      "- Each term is the sum of the two preceding ones\n",
      "- Stops when the next term would exceed 100\n",
      "- Efficient: only two variables used (a, b)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Let the agent automatically generate and execute Python code\n",
    "calculation_query = \"\"\"\n",
    "Calculate the Fibonacci sequence up to 100.\n",
    "Use an efficient iterative approach and print the result as a JSON list.\n",
    "\n",
    "Along with the results, please provide the code.\n",
    "\"\"\"\n",
    "\n",
    "python_reply, _ = client.chat_continue(MODEL,session_id,  calculation_query)\n",
    "print(\"Python Code Generation Response:\")\n",
    "print(python_reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15) Python Code Generation - Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Analysis Code Response:\n",
      "## Results\n",
      "\n",
      "**Top 3 Products by Revenue:**\n",
      "\n",
      "| Rank | Product | Total Revenue |\n",
      "|------|---------|---------------|\n",
      "| 1 | Product C | **$10,614.66** |\n",
      "| 2 | Product A | **$7,958.42** |\n",
      "| 3 | Product B | **$6,811.33** |\n",
      "\n",
      "**Total Revenue:** $25,384.40 across all products\n",
      "\n",
      "**Data Summary:**\n",
      "- 100 rows of random sales data\n",
      "- Date range: 2025\n",
      "- Products: A, B, C\n",
      "- Quantity: 1-10 units\n",
      "- Price: $10-$100\n",
      "\n",
      "---\n",
      "\n",
      "## Python Code Used\n",
      "\n",
      "The script was saved as `sales_analysis.py` and executed successfully. Here's the code:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from datetime import datetime, timedelta\n",
      "\n",
      "# Set random seed for reproducibility\n",
      "np.random.seed(42)\n",
      "\n",
      "# Generate random dates (100 days in 2025)\n",
      "dates = [datetime(2025, 1, 1) + timedelta(days=i) for i in range(100)]\n",
      "\n",
      "# Generate random product categories\n",
      "products = np.random.choice(['A', 'B', 'C'], size=100)\n",
      "\n",
      "# Generate random quantities (1-10)\n",
      "quantities = np.random.randint(1, 11, size=100)\n",
      "\n",
      "# Generate random prices ($10-$100)\n",
      "prices = np.random.uniform(10, 100, size=100)\n",
      "\n",
      "# Create DataFrame\n",
      "df = pd.DataFrame({\n",
      "    'date': dates,\n",
      "    'product': products,\n",
      "    'quantity': quantities,\n",
      "    'price': prices\n",
      "})\n",
      "\n",
      "# Calculate revenue per transaction\n",
      "df['revenue'] = df['quantity'] * df['price']\n",
      "\n",
      "# Calculate total revenue per product\n",
      "revenue_per_product = df.groupby('product')['revenue'].sum().reset_index()\n",
      "revenue_per_product.columns = ['product', 'total_revenue']\n",
      "\n",
      "# Find top 3 products by revenue\n",
      "top_products = revenue_per_product.nlargest(3, 'total_revenue')\n",
      "\n",
      "# Output results as JSON\n",
      "output = {\n",
      "    'top_3_products_by_revenue': top_products.to_dict(orient='records'),\n",
      "    'total_revenue_per_product': revenue_per_product.to_dict(orient='records'),\n",
      "    'total_revenue': float(df['revenue'].sum())\n",
      "}\n",
      "\n",
      "import json\n",
      "with open('sales_results.json', 'w') as f:\n",
      "    json.dump(output, f, indent=2)\n",
      "\n",
      "print(\"Analysis complete! Results saved to sales_results.json\")\n",
      "```\n",
      "\n",
      "**Results saved to:** `sales_results.json`\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate code to analyze data with pandas\n",
    "data_analysis_query = \"\"\"\n",
    "Write Python code to:\n",
    "1. Create a pandas DataFrame with 100 rows of random sales data (date, product, quantity, price)\n",
    "2. Calculate total revenue per product\n",
    "3. Find the top 3 products by revenue\n",
    "4. Output results as JSON\n",
    "\n",
    "Use numpy for random data generation.\n",
    "\"\"\"\n",
    "\n",
    "data_reply, _ = client.chat_continue(MODEL, session_id, data_analysis_query)\n",
    "print(\"Data Analysis Code Response:\")\n",
    "print(data_reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16) Python Code Generation - Mathematical Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mathematical Computation Response:\n",
      "## Results\n",
      "\n",
      "**First 20 prime numbers:** 2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71\n",
      "\n",
      "**Statistics:**\n",
      "- **Sum:** 639\n",
      "- **Average:** 31.95\n",
      "- **Largest:** 71\n",
      "\n",
      "---\n",
      "\n",
      "## JSON Output\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"primes\": [\n",
      "    2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71\n",
      "  ],\n",
      "  \"sum\": 639,\n",
      "  \"average\": 31.95,\n",
      "  \"largest\": 71\n",
      "}\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## Python Code Used\n",
      "\n",
      "```python\n",
      "def is_prime(n):\n",
      "    \"\"\"Check if a number is prime.\"\"\"\n",
      "    if n < 2:\n",
      "        return False\n",
      "    if n == 2:\n",
      "        return True\n",
      "    if n % 2 == 0:\n",
      "        return False\n",
      "    for i in range(3, int(n**0.5) + 1, 2):\n",
      "        if n % i == 0:\n",
      "            return False\n",
      "    return True\n",
      "\n",
      "def generate_first_n_primes(n):\n",
      "    \"\"\"Generate the first n prime numbers.\"\"\"\n",
      "    primes = []\n",
      "    num = 2\n",
      "    while len(primes) < n:\n",
      "        if is_prime(num):\n",
      "            primes.append(num)\n",
      "        num += 1\n",
      "    return primes\n",
      "\n",
      "# Generate first 20 primes\n",
      "primes = generate_first_n_primes(20)\n",
      "\n",
      "# Calculate sum and average\n",
      "total_sum = sum(primes)\n",
      "average = total_sum / len(primes)\n",
      "largest = max(primes)\n",
      "\n",
      "# Output results as JSON\n",
      "import json\n",
      "result = {\n",
      "    \"primes\": primes,\n",
      "    \"sum\": total_sum,\n",
      "    \"average\": average,\n",
      "    \"largest\": largest\n",
      "}\n",
      "\n",
      "print(json.dumps(result, indent=2))\n",
      "```\n",
      "\n",
      "**Results saved to:** `complex_json.json`\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate code for complex mathematical calculations\n",
    "math_query = \"\"\"\n",
    "Write Python code to:\n",
    "1. Calculate the first 20 prime numbers\n",
    "2. Compute their sum and average\n",
    "3. Find the largest prime in the list\n",
    "4. Output results as JSON with keys: primes, sum, average, largest\n",
    "\n",
    "Show the code.\n",
    "\"\"\"\n",
    "\n",
    "math_reply, _ = client.chat_continue(MODEL, session_id, math_query, agent_type=\"auto\")\n",
    "print(\"Mathematical Computation Response:\")\n",
    "print(math_reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17) Python Code Generation - String Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Processing Response:\n",
      "## Results\n",
      "\n",
      "**Text Analysis for:** \"The quick brown fox jumps over the lazy dog. The dog was not amused.\"\n",
      "\n",
      "| Metric | Value |\n",
      "|--------|-------|\n",
      "| **Total Word Count** | 14 |\n",
      "| **Unique Word Count** | 11 |\n",
      "| **Most Frequent Word** | \"the\" |\n",
      "| **Frequency of \"the\"** | 3 |\n",
      "| **Average Word Length** | 3.79 |\n",
      "\n",
      "---\n",
      "\n",
      "## JSON Output\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"total_word_count\": 14,\n",
      "  \"unique_word_count\": 11,\n",
      "  \"most_frequent_word\": \"the\",\n",
      "  \"frequency_of_most_frequent\": 3,\n",
      "  \"average_word_length\": 3.7857\n",
      "}\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## Python Code Used\n",
      "\n",
      "```python\n",
      "text = \"The quick brown fox jumps over the lazy dog. The dog was not amused.\"\n",
      "\n",
      "# 1. Total word count\n",
      "words = text.split()\n",
      "total_word_count = len(words)\n",
      "\n",
      "# 2. Unique word count\n",
      "unique_words = set(words)\n",
      "unique_word_count = len(unique_words)\n",
      "\n",
      "# 3. Most frequent word\n",
      "word_counts = {}\n",
      "for word in words:\n",
      "    word_counts[word] = word_counts.get(word, 0) + 1\n",
      "most_frequent_word = max(word_counts, key=word_counts.get)\n",
      "frequency_of_most_frequent = word_counts[most_frequent_word]\n",
      "\n",
      "# 4. Average word length\n",
      "total_characters = sum(len(word) for word in words)\n",
      "average_word_length = total_characters / total_word_count\n",
      "\n",
      "# 5. Output as JSON\n",
      "import json\n",
      "result = {\n",
      "    \"total_word_count\": total_word_count,\n",
      "    \"unique_word_count\": unique_word_count,\n",
      "    \"most_frequent_word\": most_frequent_word,\n",
      "    \"frequency_of_most_frequent\": frequency_of_most_frequent,\n",
      "    \"average_word_length\": average_word_length\n",
      "}\n",
      "\n",
      "with open('results.json', 'w') as f:\n",
      "    json.dump(result, f, indent=2)\n",
      "\n",
      "print(\"Analysis complete! Results saved to results.json\")\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate code for text analysis\n",
    "text_query = \"\"\"\n",
    "Write Python code to analyze the following text:\n",
    "\"The quick brown fox jumps over the lazy dog. The dog was not amused.\"\n",
    "\n",
    "Calculate:\n",
    "1. Total word count\n",
    "2. Unique word count\n",
    "3. Most frequent word\n",
    "4. Average word length\n",
    "5. Output as JSON\n",
    "\"\"\"\n",
    "\n",
    "text_reply, _ = client.chat_continue(MODEL, session_id, text_query, agent_type=\"auto\")\n",
    "print(\"Text Processing Response:\")\n",
    "print(text_reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18) Python Code Generation - Excel File Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19) File Upload with Chat - CSV Analysis\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=== Testing File Upload with Chat ===\\n\")\n",
    "\n",
    "# Create test CSV file\n",
    "test_data = pd.DataFrame({\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"],\n",
    "    \"age\": [30, 25, 35, 28, 32],\n",
    "    \"city\": [\"Seoul\", \"Busan\", \"Incheon\", \"Daegu\", \"Seoul\"],\n",
    "    \"salary\": [50000, 45000, 55000, 48000, 52000]\n",
    "})\n",
    "test_csv_path = \"test_employee_data.csv\"\n",
    "test_data.to_csv(test_csv_path, index=False)\n",
    "print(f\"✓ Created test file: {test_csv_path}\")\n",
    "print(f\"  Data shape: {test_data.shape}\")\n",
    "\n",
    "# Send chat with file attachment\n",
    "query = \"\"\"\n",
    "Analyze the attached employee CSV file and tell me:\n",
    "1. Average age and salary\n",
    "2. City with most employees\n",
    "3. Highest and lowest salary\n",
    "4. Any interesting patterns\n",
    "\"\"\"\n",
    "\n",
    "reply, session_id = client.chat_continue(\n",
    "    model=MODEL,\n",
    "    session_id = session_id, \n",
    "    user_message=query,\n",
    "    agent_type=\"auto\",\n",
    "    files=[test_csv_path]\n",
    ")\n",
    "\n",
    "print(\"\\n=== AI Response ===\")\n",
    "print(reply)\n",
    "print(f\"\\nSession ID: {session_id}\")\n",
    "\n",
    "# Cleanup\n",
    "Path(test_csv_path).unlink()\n",
    "print(f\"\\n✓ Cleaned up test file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19) File Upload with Chat - CSV Data Analysis\n",
    "\n",
    "Test the new file upload feature by creating and analyzing a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18) Python Code Generation - Excel File Analysis (with File Upload)\n",
    "# Now using the new file attachment feature!\n",
    "\n",
    "excel_path = f\"data/uploads/{username}/폴드긍정.xlsx\"\n",
    "\n",
    "excel_analysis_query = \"\"\"\n",
    "Analyze the attached Excel file and:\n",
    "1. What's the most widely appreciated feature?\n",
    "2. What's the most widely used phrase?\n",
    "3. What's the least used phrase?\n",
    "\n",
    "Make sure to handle Korean text encoding properly.\n",
    "\"\"\"\n",
    "\n",
    "# NEW: Upload the file with the chat request\n",
    "excel_reply, _ = client.chat_continue(\n",
    "    MODEL, \n",
    "    session_id, \n",
    "    excel_analysis_query, \n",
    "    agent_type=\"auto\",\n",
    "    files=[excel_path]  # Attach the file!\n",
    ")\n",
    "\n",
    "print(\"Excel File Analysis Response:\")\n",
    "print(excel_reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20. 끝말잇기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '끝말 잇기하자 나부터 시작할게: 이시훈'\n",
    "\n",
    "reply, _ = client.chat_continue(MODEL, session_id, query)\n",
    "print(reply)\n",
    "\n",
    "for i in range(10):\n",
    "    reply, _ = client.chat_continue(MODEL, session_id, query)\n",
    "    print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "question = \"Explain quantum computing in 3 sentences\"\n",
    "\n",
    "# Test non-streaming\n",
    "print(\"=== Non-Streaming (traditional) ===\")\n",
    "start = time.time()\n",
    "response, _ = client.chat_new(MODEL, question)\n",
    "elapsed_non_stream = time.time() - start\n",
    "print(f\"Response: {response}\")\n",
    "print(f\"⏱ Total time: {elapsed_non_stream:.2f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Test streaming\n",
    "print(\"=== Streaming (real-time) ===\")\n",
    "start = time.time()\n",
    "first_token_time = None\n",
    "print(\"Response: \", end=\"\", flush=True)\n",
    "\n",
    "for i, token in enumerate(client.chat_new_streaming(MODEL, question)):\n",
    "    if not token.startswith(\"SESSION_ID:\"):\n",
    "        if first_token_time is None:\n",
    "            first_token_time = time.time() - start\n",
    "        print(token, end=\"\", flush=True)\n",
    "\n",
    "elapsed_stream = time.time() - start\n",
    "\n",
    "print(f\"\\n\\n⏱ Time to first token: {first_token_time:.2f}s\")\n",
    "print(f\"⏱ Total time: {elapsed_stream:.2f}s\")\n",
    "print(f\"\\n✓ Streaming advantage: User sees response {first_token_time:.2f}s faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 23) Streaming vs Non-Streaming Comparison\n",
    "\n",
    "Compare streaming and non-streaming response times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Continue streaming conversation\n",
    "if session_id:\n",
    "    print(\"=== Continuing Conversation (Streaming) ===\\n\")\n",
    "    print(\"Question: What happened next in the story?\\n\")\n",
    "    print(\"Response: \", end=\"\", flush=True)\n",
    "    \n",
    "    for token in client.chat_continue_streaming(MODEL, session_id, \"What happened next in the story?\"):\n",
    "        print(token, end=\"\", flush=True)\n",
    "    \n",
    "    print(\"\\n\\n✓ Stream complete!\")\n",
    "else:\n",
    "    print(\"⚠ No session_id from previous example - run the previous cell first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22) Streaming Response - Continue Conversation\n",
    "\n",
    "Continue the conversation from the previous streaming example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Basic streaming with visual feedback\n",
    "print(\"=== Streaming Chat Example ===\\n\")\n",
    "print(\"Question: Tell me a short story about a robot learning to cook\\n\")\n",
    "print(\"Response: \", end=\"\", flush=True)\n",
    "\n",
    "session_id = None\n",
    "full_response = []\n",
    "\n",
    "for token in client.chat_new_streaming(MODEL, \"Tell me a short story about a robot learning to cook\"):\n",
    "    # Check if this is the session_id\n",
    "    if token.startswith(\"SESSION_ID:\"):\n",
    "        session_id = token.replace(\"SESSION_ID:\", \"\")\n",
    "    else:\n",
    "        # It's a content token\n",
    "        print(token, end=\"\", flush=True)\n",
    "        full_response.append(token)\n",
    "\n",
    "print(f\"\\n\\n✓ Stream complete!\")\n",
    "print(f\"✓ Session ID: {session_id}\")\n",
    "print(f\"✓ Total characters received: {len(''.join(full_response))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21) Streaming Response Example - Simple Chat\n",
    "\n",
    "Test the new streaming feature for real-time token-by-token responses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
