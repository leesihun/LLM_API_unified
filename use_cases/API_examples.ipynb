{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM API – End-to-End Examples (Single Notebook)\n",
    "\n",
    "This notebook shows a minimal client and step-by-step examples for:\n",
    "\n",
    "1. Create a new account\n",
    "2. Login\n",
    "3. Change models (admin privilege example)\n",
    "4. Start a new chat and get a response\n",
    "5. Continue a chat\n",
    "6. See chat history\n",
    "7. Websearch with agentic tool selection\n",
    "8. Agentic math calculation (LLM decides to use math tool)\n",
    "9. Sequential reasoning with ReAct agent (step-by-step thinking)\n",
    "10. Plan-and-Execute agent (parallel tool usage)\n",
    "11. Auto agent selection (smart router picks best agent)\n",
    "12. Complex JSON data analysis\n",
    "13. Real Data Analysis - Warpage Statistics (using 20251013_stats.json)\n",
    "14. Python Code Generation - Simple Calculation\n",
    "15. Python Code Generation - Data Analysis\n",
    "16. Python Code Generation - Mathematical Computation\n",
    "17. Python Code Generation - String Processing\n",
    "18. Python Code Generation - Excel File Analysis (Real File)\n",
    "\n",
    "Set your API base URL below if different from the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# !{sys.executable} -m pip install httpx\n",
    "# ! pip install pip-system-certs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API_BASE_URL = \"http://10.252.38.241:10007\"\n",
    "API_BASE_URL = 'http://localhost:10007'\n",
    "print(\"Using:\", API_BASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Iterator\n",
    "\n",
    "class LLMApiClient:\n",
    "    def __init__(self, base_url: str, timeout: float = 3600.0):\n",
    "        self.base_url = base_url.rstrip(\"/\")\n",
    "        self.token = None\n",
    "        # Create timeout config: 10s for connect, custom timeout for read/write/pool\n",
    "        self.timeout = httpx.Timeout(50.0, read=timeout, write=timeout, pool=timeout)\n",
    "\n",
    "    def _headers(self):\n",
    "        # Don't set Content-Type - httpx auto-sets for multipart\n",
    "        h = {}\n",
    "        if self.token:\n",
    "            h[\"Authorization\"] = f\"Bearer {self.token}\"\n",
    "        return h\n",
    "\n",
    "    def signup(self, username: str, password: str, role: str = \"guest\"):\n",
    "        r = httpx.post(f\"{self.base_url}/api/auth/signup\", json={\n",
    "            \"username\": username, \"password\": password, \"role\": role\n",
    "        }, timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "\n",
    "    def login(self, username: str, password: str):\n",
    "        r = httpx.post(f\"{self.base_url}/api/auth/login\", json={\n",
    "            \"username\": username, \"password\": password\n",
    "        }, timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        self.token = data[\"access_token\"]\n",
    "        return data\n",
    "\n",
    "    def list_models(self):\n",
    "        # JSON endpoints still use Content-Type header\n",
    "        headers = {\"Authorization\": f\"Bearer {self.token}\"} if self.token else {}\n",
    "        r = httpx.get(f\"{self.base_url}/v1/models\", headers=headers, timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "\n",
    "    def change_model(self, model: str):\n",
    "        headers = {\"Authorization\": f\"Bearer {self.token}\", \"Content-Type\": \"application/json\"} if self.token else {\"Content-Type\": \"application/json\"}\n",
    "        r = httpx.post(f\"{self.base_url}/api/admin/model\", json={\"model\": model}, headers=headers, timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "\n",
    "    def chat_new(self, model: str, user_message: str, agent_type: str = \"auto\", files: list = None):\n",
    "        messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "        \n",
    "        # Prepare form data\n",
    "        data = {\n",
    "            \"model\": model,\n",
    "            \"messages\": json.dumps(messages),\n",
    "            \"agent_type\": agent_type\n",
    "        }\n",
    "        \n",
    "        # Prepare files for upload\n",
    "        files_to_upload = []\n",
    "        if files:\n",
    "            for file_path in files:\n",
    "                f = open(file_path, \"rb\")\n",
    "                files_to_upload.append((\"files\", (Path(file_path).name, f)))\n",
    "        \n",
    "        try:\n",
    "            r = httpx.post(\n",
    "                f\"{self.base_url}/v1/chat/completions\",\n",
    "                data=data,\n",
    "                files=files_to_upload if files_to_upload else None,\n",
    "                headers=self._headers(),\n",
    "                timeout=self.timeout\n",
    "            )\n",
    "            r.raise_for_status()\n",
    "            result = r.json()\n",
    "            return result[\"choices\"][0][\"message\"][\"content\"], result[\"x_session_id\"]\n",
    "        \n",
    "        finally:\n",
    "            # Close file handles\n",
    "            for _, (_, f) in files_to_upload:\n",
    "                f.close()\n",
    "\n",
    "    def chat_new_streaming(self, model: str, user_message: str, agent_type: str = \"auto\", files: list = None) -> Iterator[str]:\n",
    "        \"\"\"\n",
    "        Start new chat with streaming response (Server-Sent Events)\n",
    "        \n",
    "        Args:\n",
    "            model: Model name\n",
    "            user_message: User message\n",
    "            agent_type: Agent type (auto, react, plan_execute) - Note: streaming only works for simple chat\n",
    "            files: Optional list of file paths to attach\n",
    "        \n",
    "        Yields:\n",
    "            Response tokens as they're generated\n",
    "            \n",
    "        Returns:\n",
    "            Iterator[str]: Yields tokens, then yields session_id as final value with prefix \"SESSION_ID:\"\n",
    "        \"\"\"\n",
    "        messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "        \n",
    "        # Prepare form data\n",
    "        data = {\n",
    "            \"model\": model,\n",
    "            \"messages\": json.dumps(messages),\n",
    "            \"agent_type\": agent_type,\n",
    "            \"stream\": \"true\"  # Enable streaming\n",
    "        }\n",
    "        \n",
    "        # Prepare files for upload\n",
    "        files_to_upload = []\n",
    "        if files:\n",
    "            for file_path in files:\n",
    "                f = open(file_path, \"rb\")\n",
    "                files_to_upload.append((\"files\", (Path(file_path).name, f)))\n",
    "        \n",
    "        try:\n",
    "            with httpx.stream(\n",
    "                \"POST\",\n",
    "                f\"{self.base_url}/v1/chat/completions\",\n",
    "                data=data,\n",
    "                files=files_to_upload if files_to_upload else None,\n",
    "                headers=self._headers(),\n",
    "                timeout=self.timeout\n",
    "            ) as response:\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                session_id = None\n",
    "                for line in response.iter_lines():\n",
    "                    if line.startswith(\"data: \"):\n",
    "                        data_str = line[6:]  # Remove \"data: \" prefix\n",
    "                        \n",
    "                        if data_str == \"[DONE]\":\n",
    "                            # Stream complete\n",
    "                            break\n",
    "                        \n",
    "                        try:\n",
    "                            chunk = json.loads(data_str)\n",
    "                            \n",
    "                            # Check for errors\n",
    "                            if \"error\" in chunk:\n",
    "                                raise Exception(f\"Streaming error: {chunk['error']['message']}\")\n",
    "                            \n",
    "                            # Extract session_id from final chunk\n",
    "                            if \"x_session_id\" in chunk:\n",
    "                                session_id = chunk[\"x_session_id\"]\n",
    "                            \n",
    "                            # Yield content delta\n",
    "                            if \"choices\" in chunk and len(chunk[\"choices\"]) > 0:\n",
    "                                delta = chunk[\"choices\"][0].get(\"delta\", {})\n",
    "                                if \"content\" in delta:\n",
    "                                    yield delta[\"content\"]\n",
    "                        \n",
    "                        except json.JSONDecodeError:\n",
    "                            # Skip malformed JSON\n",
    "                            continue\n",
    "                \n",
    "                # Yield session_id at the end with special prefix\n",
    "                if session_id:\n",
    "                    yield f\"SESSION_ID:{session_id}\"\n",
    "        \n",
    "        finally:\n",
    "            # Close file handles\n",
    "            for _, (_, f) in files_to_upload:\n",
    "                f.close()\n",
    "\n",
    "    def chat_continue(self, model: str, session_id: str, user_message: str, agent_type: str = \"auto\", files: list = None):\n",
    "        \"\"\"\n",
    "        Continue existing chat with optional file attachments (multipart/form-data)\n",
    "        \n",
    "        Args:\n",
    "            model: Model name\n",
    "            session_id: Session ID to continue\n",
    "            user_message: User message\n",
    "            agent_type: Agent type (auto, react, plan_execute)\n",
    "            files: Optional list of file paths to attach\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (response_text, session_id)\n",
    "        \"\"\"\n",
    "        messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "        \n",
    "        data = {\n",
    "            \"model\": model,\n",
    "            \"messages\": json.dumps(messages),\n",
    "            \"session_id\": session_id,\n",
    "            \"agent_type\": agent_type\n",
    "        }\n",
    "        \n",
    "        files_to_upload = []\n",
    "        if files:\n",
    "            for file_path in files:\n",
    "                f = open(file_path, \"rb\")\n",
    "                files_to_upload.append((\"files\", (Path(file_path).name, f)))\n",
    "        \n",
    "        try:\n",
    "            r = httpx.post(\n",
    "                f\"{self.base_url}/v1/chat/completions\",\n",
    "                data=data,\n",
    "                files=files_to_upload if files_to_upload else None,\n",
    "                headers=self._headers(),\n",
    "                timeout=self.timeout\n",
    "            )\n",
    "            r.raise_for_status()\n",
    "            result = r.json()\n",
    "            return result[\"choices\"][0][\"message\"][\"content\"], result[\"x_session_id\"]\n",
    "        \n",
    "        finally:\n",
    "            for _, (_, f) in files_to_upload:\n",
    "                f.close()\n",
    "\n",
    "    def chat_continue_streaming(self, model: str, session_id: str, user_message: str, agent_type: str = \"auto\", files: list = None) -> Iterator[str]:\n",
    "        \"\"\"\n",
    "        Continue existing chat with streaming response\n",
    "        \n",
    "        Args:\n",
    "            model: Model name\n",
    "            session_id: Session ID to continue\n",
    "            user_message: User message\n",
    "            agent_type: Agent type (auto, react, plan_execute) - Note: streaming only works for simple chat\n",
    "            files: Optional list of file paths to attach\n",
    "        \n",
    "        Yields:\n",
    "            Response tokens as they're generated\n",
    "        \"\"\"\n",
    "        messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "        \n",
    "        data = {\n",
    "            \"model\": model,\n",
    "            \"messages\": json.dumps(messages),\n",
    "            \"session_id\": session_id,\n",
    "            \"agent_type\": agent_type,\n",
    "            \"stream\": \"true\"\n",
    "        }\n",
    "        \n",
    "        files_to_upload = []\n",
    "        if files:\n",
    "            for file_path in files:\n",
    "                f = open(file_path, \"rb\")\n",
    "                files_to_upload.append((\"files\", (Path(file_path).name, f)))\n",
    "        \n",
    "        try:\n",
    "            with httpx.stream(\n",
    "                \"POST\",\n",
    "                f\"{self.base_url}/v1/chat/completions\",\n",
    "                data=data,\n",
    "                files=files_to_upload if files_to_upload else None,\n",
    "                headers=self._headers(),\n",
    "                timeout=self.timeout\n",
    "            ) as response:\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                for line in response.iter_lines():\n",
    "                    if line.startswith(\"data: \"):\n",
    "                        data_str = line[6:]\n",
    "                        \n",
    "                        if data_str == \"[DONE]\":\n",
    "                            break\n",
    "                        \n",
    "                        try:\n",
    "                            chunk = json.loads(data_str)\n",
    "                            \n",
    "                            if \"error\" in chunk:\n",
    "                                raise Exception(f\"Streaming error: {chunk['error']['message']}\")\n",
    "                            \n",
    "                            if \"choices\" in chunk and len(chunk[\"choices\"]) > 0:\n",
    "                                delta = chunk[\"choices\"][0].get(\"delta\", {})\n",
    "                                if \"content\" in delta:\n",
    "                                    yield delta[\"content\"]\n",
    "                        \n",
    "                        except json.JSONDecodeError:\n",
    "                            continue\n",
    "        \n",
    "        finally:\n",
    "            for _, (_, f) in files_to_upload:\n",
    "                f.close()\n",
    "\n",
    "    def chat_sessions(self):\n",
    "        r = httpx.get(f\"{self.base_url}/api/chat/sessions\", headers=self._headers(), timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()[\"sessions\"]\n",
    "\n",
    "    def chat_history(self, session_id: str):\n",
    "        r = httpx.get(f\"{self.base_url}/api/chat/history/{session_id}\", headers=self._headers(), timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()[\"messages\"]\n",
    "\n",
    "    def tools(self):\n",
    "        r = httpx.get(f\"{self.base_url}/api/tools/list\", headers=self._headers(), timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()[\"tools\"]\n",
    "\n",
    "    def websearch(self, query: str, max_results: int = 5):\n",
    "        \"\"\"\n",
    "        Perform web search and get LLM-generated answer\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with:\n",
    "            - answer: LLM-generated answer from search results\n",
    "            - results: Raw search results (list of dicts with title, url, content, score)\n",
    "            - sources_used: List of URLs used as sources\n",
    "        \"\"\"\n",
    "        headers = {\"Authorization\": f\"Bearer {self.token}\", \"Content-Type\": \"application/json\"} if self.token else {\"Content-Type\": \"application/json\"}\n",
    "        # Increased timeout to 1 hour (3600s) for web search + LLM answer generation\n",
    "        r = httpx.post(f\"{self.base_url}/api/tools/websearch\", json={\"query\": query, \"max_results\": max_results}, headers=headers, timeout=3600.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()  # Returns full response with answer, results, and sources_used\n",
    "\n",
    "    def answer_from_json(self, model: str, json_blob: dict, question: str):\n",
    "        prompt = f\"Given this JSON: {json_blob}\\nAnswer: {question}\"\n",
    "        return self.chat_new(model, prompt)[0]\n",
    "\n",
    "client = LLMApiClient(API_BASE_URL, timeout=3600.0)  # 1 hour timeout\n",
    "print(\"Client ready with 3600s (1 hour) timeout for all requests\")\n",
    "print(\"✓ Now supports multipart/form-data with optional file attachments\")\n",
    "print(\"✓ Now supports streaming responses via chat_new_streaming() and chat_continue_streaming()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Create a new account (skip if user already exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "username = \"leesihun\"\n",
    "password = \"s.hun.lee\"\n",
    "try:\n",
    "    result = client.signup(username, password)\n",
    "    print(f\"Account created: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"Signup skipped (user may already exist): {e}\")\n",
    "    print(\"Continuing with existing account...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "login = client.login(username, password)\n",
    "login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Change models (admin only) – optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using deepseek-r1:1.5b for all examples\n",
    "client.login(\"admin\", \"administrator\")\n",
    "# client.change_model(\"gpt-oss:120b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List models (OpenAI-compatible)\n",
    "models = client.list_models()\n",
    "models\n",
    "\n",
    "MODEL = models[\"data\"][0][\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Start a new chat and get a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply, session_id = client.chat_new(MODEL, \"Hello! Give me a short haiku about autumn.\")\n",
    "reply, session_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Continue an existing chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reply2, _ = client.chat_continue(MODEL, session_id, \"Now do one about winter.\")\n",
    "reply2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) See chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client.chat_sessions(), client.chat_history(session_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Websearch with LLM-generated answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # The API now generates an answer from search results using LLM\n",
    "# client.login(\"leesihun\", \"s.hun.lee\")\n",
    "# search_query = \"Tell me who is SiHun Lee\"\n",
    "# search_response, _ = client.chat_continue(MODEL, session_id, search_query, agent_type = 'react')\n",
    "\n",
    "# print(\"=== LLM-Generated Answer ===\")\n",
    "# print(search_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 7b) Websearch example - Sports news\n",
    "# # Another example showing the LLM answer generation\n",
    "# client.login(\"leesihun\", \"s.hun.lee\")\n",
    "# search_query = \"What was the latest game of Liverpool FC and who won? The current date is 2025/12/12\"\n",
    "# search_response, _ = client.chat_new(MODEL, search_query, agent_type = 'react')\n",
    "\n",
    "# print(\"=== LLM-Generated Answer ===\")\n",
    "# print(search_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) Agentic tool usage - Let the LLM decide which tool to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math_reply, _ = client.chat_continue(MODEL, session_id, \"What is 11.951/3.751?\", agent_type='react')\n",
    "print(\"Math Question Response:\")\n",
    "from IPython.display import display, Math, Latex\n",
    "display(Latex(math_reply))\n",
    "print(math_reply)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9) Sequential reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This triggers the ReAct agent because it requires step-by-step thinking\n",
    "sequential_query = \"\"\"\n",
    "First, search the web to find the latest population of Tokyo.\n",
    "Then, calculate what 15% of that population would be.\n",
    "Finally, tell me the result.\n",
    "Think hard, try to answer to best of your knowledge\n",
    "\"\"\"\n",
    "react_reply, _ = client.chat_continue(MODEL, session_id, sequential_query)\n",
    "print(\"Sequential Reasoning (ReAct) Response:\")\n",
    "\n",
    "display(Latex(react_reply))\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10) Plan-and-Execute agent with multiple tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This triggers Plan-and-Execute agent because it uses \"and\" for parallel tasks\n",
    "parallel_query = \"\"\"\n",
    "Search for the latest news about artificial intelligence and\n",
    "calculate the result of (100 * 0.15 + 25) / 2 and\n",
    "Think about what god is and\n",
    "What the best smart phone is and\n",
    "what is 1007*1007/4524753.\n",
    "\"\"\"\n",
    "plan_reply, _ = client.chat_continue(MODEL,session_id,  parallel_query, agent_type=\"plan_execute\")\n",
    "print(\"Plan-and-Execute Response:\")\n",
    "\n",
    "display(Latex(plan_reply))\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11) Auto agent selection - Let the router decide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The smart router will analyze the query and pick the best agent\n",
    "auto_query = \"If the capital of France has a population of 2.1 million, and we need to allocate 500 euros per person for a project, what's the total budget needed? First search for the actual population, then calculate.\"\n",
    "auto_reply, _ = client.chat_continue(MODEL, session_id, auto_query, agent_type=\"auto\")\n",
    "print(\"Auto Agent Selection Response:\")\n",
    "print(auto_reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12) Complex JSON data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12) Complex JSON data analysis (with File Upload)\n",
    "import json\n",
    "\n",
    "# Create a realistic e-commerce dataset\n",
    "complex_json = {\n",
    "    \"company\": \"TechMart Inc\",\n",
    "    \"quarter\": \"Q3 2025\",\n",
    "    \"departments\": [\n",
    "        {\n",
    "            \"name\": \"Electronics\",\n",
    "            \"employees\": 45,\n",
    "            \"sales\": [\n",
    "                {\"product\": \"Laptop\", \"units_sold\": 320, \"price\": 1200, \"revenue\": 384000},\n",
    "                {\"product\": \"Smartphone\", \"units_sold\": 856, \"price\": 800, \"revenue\": 684800},\n",
    "                {\"product\": \"Tablet\", \"units_sold\": 142, \"price\": 500, \"revenue\": 71000}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Home Appliances\",\n",
    "            \"employees\": 32,\n",
    "            \"sales\": [\n",
    "                {\"product\": \"Refrigerator\", \"units_sold\": 89, \"price\": 1500, \"revenue\": 133500},\n",
    "                {\"product\": \"Washing Machine\", \"units_sold\": 124, \"price\": 900, \"revenue\": 111600},\n",
    "                {\"product\": \"Microwave\", \"units_sold\": 267, \"price\": 200, \"revenue\": 53400}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Furniture\",\n",
    "            \"employees\": 28,\n",
    "            \"sales\": [\n",
    "                {\"product\": \"Desk\", \"units_sold\": 178, \"price\": 450, \"revenue\": 80100},\n",
    "                {\"product\": \"Chair\", \"units_sold\": 432, \"price\": 150, \"revenue\": 64800},\n",
    "                {\"product\": \"Bookshelf\", \"units_sold\": 95, \"price\": 300, \"revenue\": 28500}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save the data to a JSON file\n",
    "json_name = './complex_json.json'\n",
    "with open(json_name, 'w') as f:\n",
    "    json.dump(complex_json, f, indent=2)\n",
    "\n",
    "# Ask the LLM to analyze the attached JSON file\n",
    "analysis_query = \"\"\"\n",
    "Analyze the attached company data JSON file and tell me:\n",
    "1. Which department has the highest total revenue?\n",
    "2. What is the average revenue per employee across all departments?\n",
    "3. Which single product generated the most revenue?\n",
    "4. Calculate the total units sold across all departments.\n",
    "\n",
    "Please provide exact numbers and show your calculations.\n",
    "\"\"\"\n",
    "\n",
    "# Expected Answers:\n",
    "# 1. Electronics: 1,139,800\n",
    "# 2. 15,356.19 (1,615,300 total revenue / 105 total employees)\n",
    "# 3. Smartphone: 684,800\n",
    "# 4. 2,503 units total\n",
    "\n",
    "# NEW: Upload the JSON file instead of pasting data in prompt\n",
    "json_reply, _ = client.chat_continue(\n",
    "    MODEL, \n",
    "    session_id,\n",
    "    analysis_query,\n",
    "    files=[json_name]\n",
    ")\n",
    "\n",
    "print(\"\\n=== Complex JSON Analysis Response ===\")\n",
    "print(json_reply)\n",
    "\n",
    "# Cleanup\n",
    "Path(json_name).unlink()\n",
    "print(f\"\\n✓ Cleaned up {json_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13) Real Data Analysis - Warpage Statistics\n",
    "\n",
    "Analyze real manufacturing warpage measurement data from uploaded JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13) Real Data Analysis - Warpage Statistics from JSON\n",
    "# Load and analyze the actual warpage analysis report data\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "stats_path = Path(f\"data/uploads/{username}/20251013_stats.json\")\n",
    "with open(stats_path, 'r') as f:\n",
    "    warpage_stats = json.load(f)\n",
    "\n",
    "analysis_query = \"\"\"\n",
    "The given file contains 50 measurements of warpage.\n",
    "Based on this warpage analysis data, please analyze and tell me:\n",
    "\n",
    "1. Which has the highest maximum warpage value and what is it?\n",
    "2. Which has the lowest minimum warpage value and what is it?\n",
    "3. What is the average mean warpage across all 50 measurements?\n",
    "4. Calculate the overall standard deviation range (min std to max std) across all measurements\n",
    "5. Which measurement shows the most variability (highest range) and what is that range?\n",
    "6. What is the average kurtosis value across all measurements?\n",
    "7. Identify any files with extreme kurtosis (>47) which might indicate outliers\n",
    "\n",
    "Please provide specific file IDs and numeric values in your analysis.\n",
    "\"\"\"\n",
    "\n",
    "warpage_reply, _ = client.chat_new(MODEL, analysis_query, files=[stats_path])\n",
    "print(\"=== Warpage Statistics Analysis ===\")\n",
    "from IPython.display import display, Math, Latex\n",
    "display(Latex(warpage_reply))\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "\"\"\"\n",
    "Answer: \n",
    "\n",
    "\n",
    "1. Highest Maximum Warpage Value:\n",
    "File ID: File_48\n",
    "Filename: 20251013146640@B5913326505D7_ORI.txt\n",
    "Maximum Value: 534.0\n",
    "2. Lowest Minimum Warpage Value:\n",
    "File ID: File_04\n",
    "Filename: 20251013142156@B5913326505D7_ORI.txt\n",
    "Minimum Value: -4200.0\n",
    "3. Average Mean Warpage:\n",
    "-297.84 (across all 50 measurements)\n",
    "4. Overall Standard Deviation Range:\n",
    "Minimum Std: 71.23\n",
    "Maximum Std: 87.89\n",
    "Range: 16.66\n",
    "5. Most Variability (Highest Range):\n",
    "File ID: File_04\n",
    "Filename: 20251013142156@B5913326505D7_ORI.txt\n",
    "Range: 4723.0\n",
    "6. Average Kurtosis Value:\n",
    "44.43 (across all measurements)\n",
    "7. Files with Extreme Kurtosis (>47) - Potential Outliers: Found 7 files with extreme kurtosis:\n",
    "File ID\tFilename\tKurtosis\n",
    "File_48\t20251013146640@...\t48.35\n",
    "File_36\t20251013145428@...\t48.23\n",
    "File_24\t20251013144216@...\t48.12\n",
    "File_04\t20251013142156@...\t47.89\n",
    "File_40\t20251013145832@...\t47.57\n",
    "File_28\t20251013144620@...\t47.46\n",
    "File_12\t20251013143004@...\t47.12\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14) Python Code Generation - Simple Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let the agent automatically generate and execute Python code\n",
    "calculation_query = \"\"\"\n",
    "Calculate the Fibonacci sequence up to 100.\n",
    "Use an efficient iterative approach and print the result as a JSON list.\n",
    "\n",
    "Along with the results, please provide the code.\n",
    "\"\"\"\n",
    "\n",
    "python_reply, _ = client.chat_continue(MODEL,session_id,  calculation_query)\n",
    "print(\"Python Code Generation Response:\")\n",
    "print(python_reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15) Python Code Generation - Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate code to analyze data with pandas\n",
    "data_analysis_query = \"\"\"\n",
    "Write Python code to:\n",
    "1. Create a pandas DataFrame with 100 rows of random sales data (date, product, quantity, price)\n",
    "2. Calculate total revenue per product\n",
    "3. Find the top 3 products by revenue\n",
    "4. Output results as JSON\n",
    "\n",
    "Use numpy for random data generation.\n",
    "\"\"\"\n",
    "\n",
    "data_reply, _ = client.chat_continue(MODEL, session_id, data_analysis_query)\n",
    "print(\"Data Analysis Code Response:\")\n",
    "print(data_reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16) Python Code Generation - Mathematical Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate code for complex mathematical calculations\n",
    "math_query = \"\"\"\n",
    "Write Python code to:\n",
    "1. Calculate the first 20 prime numbers\n",
    "2. Compute their sum and average\n",
    "3. Find the largest prime in the list\n",
    "4. Output results as JSON with keys: primes, sum, average, largest\n",
    "\n",
    "Show the code.\n",
    "\"\"\"\n",
    "\n",
    "math_reply, _ = client.chat_continue(MODEL, session_id, math_query, agent_type=\"auto\")\n",
    "print(\"Mathematical Computation Response:\")\n",
    "print(math_reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17) Python Code Generation - String Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate code for text analysis\n",
    "text_query = \"\"\"\n",
    "Write Python code to analyze the following text:\n",
    "\"The quick brown fox jumps over the lazy dog. The dog was not amused.\"\n",
    "\n",
    "Calculate:\n",
    "1. Total word count\n",
    "2. Unique word count\n",
    "3. Most frequent word\n",
    "4. Average word length\n",
    "5. Output as JSON\n",
    "\"\"\"\n",
    "\n",
    "text_reply, _ = client.chat_continue(MODEL, session_id, text_query, agent_type=\"auto\")\n",
    "print(\"Text Processing Response:\")\n",
    "print(text_reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18) Python Code Generation - Excel File Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19) File Upload with Chat - CSV Analysis\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=== Testing File Upload with Chat ===\\n\")\n",
    "\n",
    "# Create test CSV file\n",
    "test_data = pd.DataFrame({\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"],\n",
    "    \"age\": [30, 25, 35, 28, 32],\n",
    "    \"city\": [\"Seoul\", \"Busan\", \"Incheon\", \"Daegu\", \"Seoul\"],\n",
    "    \"salary\": [50000, 45000, 55000, 48000, 52000]\n",
    "})\n",
    "test_csv_path = \"test_employee_data.csv\"\n",
    "test_data.to_csv(test_csv_path, index=False)\n",
    "print(f\"✓ Created test file: {test_csv_path}\")\n",
    "print(f\"  Data shape: {test_data.shape}\")\n",
    "\n",
    "# Send chat with file attachment\n",
    "query = \"\"\"\n",
    "Analyze the attached employee CSV file and tell me:\n",
    "1. Average age and salary\n",
    "2. City with most employees\n",
    "3. Highest and lowest salary\n",
    "4. Any interesting patterns\n",
    "\"\"\"\n",
    "\n",
    "reply, session_id = client.chat_continue(\n",
    "    model=MODEL,\n",
    "    session_id = session_id, \n",
    "    user_message=query,\n",
    "    agent_type=\"auto\",\n",
    "    files=[test_csv_path]\n",
    ")\n",
    "\n",
    "print(\"\\n=== AI Response ===\")\n",
    "print(reply)\n",
    "print(f\"\\nSession ID: {session_id}\")\n",
    "\n",
    "# Cleanup\n",
    "Path(test_csv_path).unlink()\n",
    "print(f\"\\n✓ Cleaned up test file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19) File Upload with Chat - CSV Data Analysis\n",
    "\n",
    "Test the new file upload feature by creating and analyzing a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18) Python Code Generation - Excel File Analysis (with File Upload)\n",
    "# Now using the new file attachment feature!\n",
    "\n",
    "excel_path = f\"data/uploads/{username}/폴드긍정.xlsx\"\n",
    "\n",
    "excel_analysis_query = \"\"\"\n",
    "Analyze the attached Excel file and:\n",
    "1. What's the most widely appreciated feature?\n",
    "2. What's the most widely used phrase?\n",
    "3. What's the least used phrase?\n",
    "\n",
    "Make sure to handle Korean text encoding properly.\n",
    "\"\"\"\n",
    "\n",
    "# NEW: Upload the file with the chat request\n",
    "excel_reply, _ = client.chat_continue(\n",
    "    MODEL, \n",
    "    session_id, \n",
    "    excel_analysis_query, \n",
    "    agent_type=\"auto\",\n",
    "    files=[excel_path]  # Attach the file!\n",
    ")\n",
    "\n",
    "print(\"Excel File Analysis Response:\")\n",
    "print(excel_reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20. 끝말잇기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '끝말 잇기하자 나부터 시작할게: 이시훈'\n",
    "\n",
    "reply, _ = client.chat_continue(MODEL, session_id, query)\n",
    "print(reply)\n",
    "\n",
    "for i in range(10):\n",
    "    reply, _ = client.chat_continue(MODEL, session_id, query)\n",
    "    print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "question = \"Explain quantum computing in 3 sentences\"\n",
    "\n",
    "# Test non-streaming\n",
    "print(\"=== Non-Streaming (traditional) ===\")\n",
    "start = time.time()\n",
    "response, _ = client.chat_new(MODEL, question)\n",
    "elapsed_non_stream = time.time() - start\n",
    "print(f\"Response: {response}\")\n",
    "print(f\"⏱ Total time: {elapsed_non_stream:.2f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Test streaming\n",
    "print(\"=== Streaming (real-time) ===\")\n",
    "start = time.time()\n",
    "first_token_time = None\n",
    "print(\"Response: \", end=\"\", flush=True)\n",
    "\n",
    "for i, token in enumerate(client.chat_new_streaming(MODEL, question)):\n",
    "    if not token.startswith(\"SESSION_ID:\"):\n",
    "        if first_token_time is None:\n",
    "            first_token_time = time.time() - start\n",
    "        print(token, end=\"\", flush=True)\n",
    "\n",
    "elapsed_stream = time.time() - start\n",
    "\n",
    "print(f\"\\n\\n⏱ Time to first token: {first_token_time:.2f}s\")\n",
    "print(f\"⏱ Total time: {elapsed_stream:.2f}s\")\n",
    "print(f\"\\n✓ Streaming advantage: User sees response {first_token_time:.2f}s faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 23) Streaming vs Non-Streaming Comparison\n",
    "\n",
    "Compare streaming and non-streaming response times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Continue streaming conversation\n",
    "if session_id:\n",
    "    print(\"=== Continuing Conversation (Streaming) ===\\n\")\n",
    "    print(\"Question: What happened next in the story?\\n\")\n",
    "    print(\"Response: \", end=\"\", flush=True)\n",
    "    \n",
    "    for token in client.chat_continue_streaming(MODEL, session_id, \"What happened next in the story?\"):\n",
    "        print(token, end=\"\", flush=True)\n",
    "    \n",
    "    print(\"\\n\\n✓ Stream complete!\")\n",
    "else:\n",
    "    print(\"⚠ No session_id from previous example - run the previous cell first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22) Streaming Response - Continue Conversation\n",
    "\n",
    "Continue the conversation from the previous streaming example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Basic streaming with visual feedback\n",
    "print(\"=== Streaming Chat Example ===\\n\")\n",
    "print(\"Question: Tell me a short story about a robot learning to cook\\n\")\n",
    "print(\"Response: \", end=\"\", flush=True)\n",
    "\n",
    "session_id = None\n",
    "full_response = []\n",
    "\n",
    "for token in client.chat_new_streaming(MODEL, \"Tell me a short story about a robot learning to cook\"):\n",
    "    # Check if this is the session_id\n",
    "    if token.startswith(\"SESSION_ID:\"):\n",
    "        session_id = token.replace(\"SESSION_ID:\", \"\")\n",
    "    else:\n",
    "        # It's a content token\n",
    "        print(token, end=\"\", flush=True)\n",
    "        full_response.append(token)\n",
    "\n",
    "print(f\"\\n\\n✓ Stream complete!\")\n",
    "print(f\"✓ Session ID: {session_id}\")\n",
    "print(f\"✓ Total characters received: {len(''.join(full_response))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21) Streaming Response Example - Simple Chat\n",
    "\n",
    "Test the new streaming feature for real-time token-by-token responses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
